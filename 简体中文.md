给极客的互联网计算机白皮书


(v1.0)

DFINITY团队[^1]

一月21日, 2022

**摘要**

智能合约是一种新的软件形式，它将彻底改变现有的软件编写方式，IT系统的维护方式，以及改变各种应用程序的构建方式甚至是整个商业的革命。智能合约是在去中心化区块链上运行的可组合的、独立自主的软件，这使得它们不可篡改且无法停止。在本文中，我们描述了互联网计算机 (IC)，它是一种全新的区块链设计，可以释放智能合约的全部潜力，克服传统区块链上智能合约在速度、存储成本和计算能力方面的限制. 这样子智能合约第一次可以实现完全去中心化的应用程序，这些应用程序端到端的托管在区块链上。 IC 由一组加密协议组成，这些协议将各个独立运营的节点连接到一个区块链集合中。这些区块链托管并执行“canister”，即 IC 的智能合约形式。Canisters可以存储数据，对该数据执行非常通用的计算，并提供完整的技术堆栈，直接为终端用户提供网页服务。计算和存储费用通过“反向GAS模型”，由canister开发人员通过cycles预付这些费用，这些cycles是由IC的原生代币 ICP 兑换获得的。 ICP代币也用于治理：IC由去中心化自治组织（DAO）管理，决定网络拓扑结构的变化、协议的升级以及其他事情。

1 介绍

1.  释放智能合约的潜力

由于其独有的特性，智能合约是 Web3 的关键推动者，Web3 是一种新的互联网方法，应用程序完全由用户控制并在去中心化的区块链平台上运行。这种去中心化应用程序（dapps）通常是代币化的，这意味着代币被分发给用户作为参与 dapps 的奖励。参与可以有多种不同的形式，从审核和提供内容到管理 dapp 以及创建和维护 dapp。通常，代币也可以在交易所购买；事实上，出售代币是给 dapp 开发提供资金的一种常见方式。最后，代币还被用作 dapp 提供的服务或内容的一种支付方式。在当今的区块链平台上运行的智能合约，包括所有流行的平台（如以太坊），都受到许多限制，例如交易和存储成本高、计算速度慢以及无法为用户提供前端服务。因此，许多流行的区块链应用程序并不是完全去中心化的，而是混合体，其中大部分应用程序托管在传统云平台上，并调用区块链上的智能合约以实现其整体功能的一小部分。不幸的是，这使得此类应用程序变得非去中心化，并让它们面临传统云托管应用程序的许多缺点，例如受云提供商的摆布，以及容易受到许多单点故障的影响。

**互联网计算机（IC）** 是执行智能合约的新平台。在这里，我们在非常广泛的意义上使用 **智能合约** 一词：一种*通用目的、不可变、防篡改的*计算机程序，在*去中心化的公共网络*上面*自主*执行。

-   *通用目的*，我们是指智能合约程序是图灵完备的那类（即任何可计算的东西都可以由智能合约计算）。

-   *不可变*，是指一旦部署，智能合约的代码不能由一方单方面更改。

-   *防篡改*，是指程序的指令得到如实执行，中间和最终结果被准确存储和/或传输。

-   *独立自主*，是指智能合约由网络自动执行，无需任何个人采取任何行动。

-   *去中心化公共网络*，是指可公开访问、地理位置分散，且不受少数个人或组织控制的计算机网络。

此外，智能合约

-   *可组合*，意味着它们可以相互交互，并且

-   支持*代币化*，意味着它们可以使用和交易数字代币。

与现有的智能合约平台相比，IC 旨在：

-   *更具成本效益*，特别是允许应用程序以先前平台成本的一小部分来计算和存储数据；

-   为处理智能合约交易提供*更高的吞吐量和更低的延迟*；

-   *更具可扩展性*，特别是，IC 可以原生的处理无限量的智能合约数据和计算，因为它可以通过向网络添加更多节点来增加容量。

除了提供智能合约平台外，IC 还被设计成一个*完整的技术堆栈，*这样可以构建完全在 IC 上运行的系统和服务。特别是，IC 上的智能合约可以服务于最终用户创建的 HTTP 请求，因此智能合约可以直接服务于交互式 Web 体验。这意味着可以在不依赖企业云托管服务或私有服务器的情况下创建系统和服务，从而以真正的端到端方式提供智能合约的所有好处。

**实现 Web3 的愿景。** 对于终端用户而言，访问基于 IC 的服务在很大程度上是透明的。他们的个人数据比访问公共或私有云上的应用程序更安全，但与应用程序交互的体验是相同的。

然而，对于创建和管理这些基于 IC 的服务的人来说，IC 消除了与开发和部署现代应用程序和微服务相关的许多成本、风险和复杂性。例如，相比正在垄断整个互联网的大型科技公司推动的整合，IC 平台为提供了另一种选择。此外，其安全协议可确保可靠的消息传递、透明的问责制和弹性，而无需依赖防火墙、备份设施、负载平衡服务或故障转移编排布置。

构建 IC 就是要让互联网恢复其开放、创新和创造性的根基 --- 换句话说，就是实现 Web3 的愿景。*为了专注于几个具体示例，IC 实现了：

-   支持互操作性、共享函数、永久 API 和无主应用程序，所有这些都可以降低平台风险并鼓励创新和协作。

-   将数据自动保存在内存中，从而消除了对数据库服务器和存储管理的需求，提高了计算效率，并简化了软件开发。

-   简化 IT 组织需要集成和管理的技术堆栈，从而提高运营效率

2.  互联网计算机的高层概括

大致上，IC 是一个交互 **复制状态机** 的网络。复制状态机的概念在分布式系统 [\[Sch90\]](\l) 中是一个相当标准的概念，但我们在这里简要介绍一下，从*状态机*的概念开始。

**状态机** 是一种特定的计算模型。这样的机器维护一个 **状态** ，它对应于普通计算机中的主存储器或其他形式的数据存储。这样的机器在离散的 **轮次** 中执行：在每一轮中，它接受一个输入，对*输入*和*当前状态*应用一个 **状态转换函数** ，获得一个 **输出** 和一个*新状态*。*新状态*成为下一轮的*当前状态*。

IC 的状态转换函数是一个 **通用函数**，这意味着存储在状态中的一些输入和数据可能是作用于其他输入和数据的任意 **程序**。因此，这样的状态机代表了一个通用的（即图灵完备的）计算模型。

为了实现 **容错**，可以将状态机复制。复制的状态机包含一个 **子网** 的 **副本** 集合，每个副本都运行同一状态机的副本。即使某些副本出现 **故障**，子网也应继续运行并正常运行。

子网中的每个副本都必须以相同的顺序处理相同的输入。为此，子网中的副本必须运行共识协议 [Fis83]，以确保子网中的所有副本以相同的顺序处理输入。因此，每个副本的内部状态将以完全相同的方式随着时间的推移而演变，并且每个副本将产生完全相同的输出序列。请注意，IC 上复制状态机的输入可能是外部用户的输入，也可能是另一个复制状态机生成的输出。类似地，复制状态机的输出可以是指向外部用户的输出，也可以是另一个复制状态机的输入。

3.  故障模型

在计算机科学的这一领域，通常会考虑两种类型的副本故障：**崩溃故障** 和 **拜占庭故障**。当副本突然停止并且没有恢复时，会发生 **崩溃故障**。 **拜占庭故障** 是副本可能以任意方式偏离其规定协议的故障。 此外，由于拜占庭式故障，一个或多个副本可能直接处于恶意对手的控制之下，该对手可能会协调这些副本的行为。 在这两种类型的故障中，拜占庭式故障可能更具破坏性。

用于共识和实现复制状态机的协议通常会假设 **有多少** 副本可能有故障，以及它们可能的故障 **类型**（崩溃或拜占庭）。 在 IC 中，假设如果给定的子网有 n 个副本，那么这些副本中有不到 n/3 是有故障的，并且这些故障可能是拜占庭的。 （请注意，IC 中不同的子网可能有不同的大小。）

4.  通信模型

用于共识和实现复制状态机的协议通常也会对 **通信模型** 做出假设，对恶意对手延迟副本之间消息传递的能力进行描述。在描述范围的两端，我们有以下模型：

• 在 **同步模型** 中，存在一些已知的有限时间限制δ，因此对于任何发送的消息，它都将在小于时间δ 内传递。

• 在 **异步模型** 中，对于发送的任何消息，攻击者可以将其传递延迟任意有限的时间，因此传递消息的时间没有限制。然而，每条消息 **最终** 都必须被传递。

由于 IC 子网中的副本通常分布在全球各地，因此同步通信模型将非常的不切实际。事实上，攻击者可以通过延迟诚实的副本或它们之间的通信来破坏协议的正确行为。这种攻击通常比控制和破坏诚实的副本更容易实施。

在一个节点分布在全球各地的子网设置中，最现实、最健壮的模型是异步模型。不幸的是，在这个模型中没有已知的真正实用的共识协议（最近的异步共识协议，如 [\[MXC+16\]](\l)，获得了不错的吞吐量，但延迟不是很好）。因此，与大多数其他不依赖同步通信的实用的拜占庭容错系统（例如，PBFT [\[CL99](\l), [BKM18,
AMN+20\]](\l)）一样，IC 选择了一种折衷方案：**部分同步** 通信模型 [\[DLS88\]](\l)。这种部分同步模型可以用各种方式来制定。 IC 使用的部分同步假设粗略地说，对于每个子网，该子网中的副本之间的通信是周期性同步的，时间间隔很短；此外，同步时限 δ 不需要事先知道。这种部分同步假设只需要确保共识协议取得进展（所谓的活性属性）。不需要部分同步假设（也不是最终消息传递假设）来确保共识的正确行为（所谓的安全属性），IC 协议栈中的其他任何地方也不需要它。

在部分同步和拜占庭故障的假设下，目前已知我们对故障数量 *f \<* n/3 的界限设置是最优的。

5.  权限模型

最早的共识协议（例如，PBFT [\[CL99\]](\l)）是**需要许可的（Permissioned）**，因为组成复制状态机的副本由一个中心化的组织管理，该组织决定哪些实体提供副本、网络的拓扑结构，以及可能实现某种中心化的公钥基础设施。虽然许可共识协议通常是最有效的，同时它们确实避免了单点故障，但中心化治理对于某些应用程序来说是不合要求的，它与蓬勃发展的 Web3 时代的精神背道而驰。

最近，我们看到了**无需许可的（Permissionless）** 共识协议的兴起，例如比特币 [\[Nak08\]](\l)、以太坊 [\[But13\]](\l) 和 Algorand [\[GHM+17\]](\l)。此类协议基于 **区块链** ，**工作量证明 (PoW)**（例如，比特币、v2.0 之前的以太坊）或 **权益证明 (PoS)**（例如，Algorand、以太坊 v2.0）。尽管此类协议是完全去中心化的，但它们的效率远低于许可协议。我们还指出，正如 [\[PSS17\]](\l) 中所观察到的，基于 PoW 的共识协议（例如比特币）不能保证异步通信网络中的正确性（即安全性）。

IC 的许可模型是一种 **混合模型** ，在获得需要许可协议的效率的同时提供去中心化 PoS 协议的许多好处。这种混合模型被称为 **DAO 控制的网络** ，并且（粗略地说）工作如下：每个子网运行一个许可的共识协议，但一个去中心化自治组织（DAO）确定哪些实体提供副本，配置网络的拓扑，提供公钥基础设施，并控制将哪个版本的协议部署到副本。 IC 的 DAO 被称为 **网络神经系统 (NNS)** ，它基于 PoS，因此 NNS 做出的所有决定都是由社区成员做出的，其投票权取决于他们质押了多少 IC 的原生治理令牌在 NNS 中（有关此令牌的更多信息，请参见第 1.8 节）。通过这个基于 PoS 的治理系统，可以创建新的子网，可以在现有子网中添加或删除副本，可以部署软件更新，并且可以对 IC 进行其他修改。 NNS 本身就是一个复制状态机，它（与任何其他状态机一样）运行在特定子网上，其子网的成员资格是通过相同的基于 PoS 的治理系统确定的。 NNS 维护一个称为 **注册表** 的数据库，该数据库跟踪 IC 的拓扑结构：哪些副本属于哪些子网、副本的公钥等。 （有关 NNS 的更多详细信息，请参见第 1.10 节。）

因此，人们看到 由 IC 的 DAO 控制的网络使 IC 获得了许可网络的许多实际好处（就更有效的共识而言），同时保持去中心化网络的许多好处（DAO治理）。

运行 IC 协议的副本托管在地理位置分散、独立运营的数据中心的服务器上。这也增强了 IC 的安全性和去中心化的性质。

6.  链密钥密码技术

事实上，IC 的共识协议确实使用了区块链，但它也使用了公钥密码学，特别是数字签名：由 NNS 维护的注册表用于将各个公钥绑定到各个副本和子网，形成一个整体。这实现了一个独特而强大的技术集合，我们称之为 **链密钥密码技术** ，它有几个组件。

1.6.1 阈值签名

链密钥密码技术的第一个组成部分是 **阈值签名**：这是一种成熟的加密技术，它允许子网拥有一个公共签名验证密钥，其对应的秘密签名密钥被分成多个部分，这些部分分布在子网的所有副本中，使不诚实的副本持有的份额不会让他们伪造任何签名，而诚实的副本持有的份额允许子网生成与 IC 的策略和协议一致的签名。

这些阈值签名的一个关键应用是

*一个子网的单个输出可以由另一个子网或外部用户通过简单地验证数字签名来验证，用（第一个）子网（在IC上面即NNS）的公共签名验证密钥。*

请注意，子网的公共签名验证密钥可以从 NNS 获得 --- 此公共签名验证密钥在子网的生命周期内保持不变（即使子网的成员可能在该生命周期内发生变化）。这与许多不可扩展的基于区块链的协议形成鲜明对比，后者需要验证整个区块链才能验证任何单个输出。

正如我们将看到的，这些阈值签名在 IC 中还有许多其他应用。一个这样的应用程序是让子网中的每个副本访问不可预测的伪随机位（源自此类签名）。这是共识中使用的 **随机信标** 和执行中使用的 **随机磁带** 的基础。

为了安全地部署阈值签名，IC 使用了一种创新的 **分布式密钥生成 (DKG)** 协议，该协议构建了一个公共签名验证密钥，并为每个副本提供相应的秘密签名密钥的一部份，并在我们的故障和通信模型中工作。

1.6.2 链进化技术

链密钥密码技术还包括一系列复杂的技术，用于随着时间的推移稳健、安全地维护基于区块链的复制状态机，这些技术共同构成了我们所说的 **链进化技术** 。每个子网根据 **epochs** 来运行（每个epoch通常在几百个区块的数量级）。使用阈值签名和许多其他技术，链演进技术实现了许多基本的维护活动，这些活动以epochs为基本单位，按一定节奏定期执行：

**垃圾收集：** 在每个 epoch 结束时，所有已处理的输入以及对这些输入排序所需的所有共识级协议消息都可以安全地从每个副本的内存中清除。这对于防止副本的存储需求无限制地增长至关重要。这与许多不可扩展的基于区块链的协议形成对比，它们必须存储从创世块开始的整个区块链。

**快速转发：** 如果子网中的一个副本远远落后于它的对等节点（因为它长时间宕机或与网络断开连接），或者一个新的副本被添加到子网中，它可以通过 **快速转发** 数据更新到最近的epoch的开始部分，无需运行共识协议并无需处理到该点之前的所有输入。这与许多不可扩展的基于区块链的协议形成对比，它们必须处理从创世块开始的整个区块链。

**子网成员变化：** 子网成员（由 NNS 确定，参见第 1.5 节）可能会随着时间而变化。这只能发生在 epoch 的边界，需要小心操作以确保一致和正确的行为。

**主动重新配发秘密：** 我们在上面的 1.6.1 节中提到了 IC 如何使用链密钥密码技术 --- 特别是阈值签名 --- 进行输出验证。这是基于 **秘密共享** 的，它通过将秘密（在本例中为秘密签名密钥）拆分为存储在副本之间的不同部分（shares）来避免任何单点故障。在每个epoch开始时，这些秘密部分(shares)都会被 **主动重新配发**。这实现了两个目标：

-   当子网的成员发生变化时，重新配发将确保任何新成员都拥有秘密恰当的一部分，而不再是成员的任何副本不再拥有秘密的一部分。

-   如果在任何一个epoch甚至是在每个epoch都有少量秘密部分被泄露给攻击者，那么这些秘密部分对攻击者没有帮助。

**协议升级：** 当 IC 协议本身需要升级、修复 bug 或添加新功能时，可以在一个 epoch 开始时使用特殊协议自动完成。

7.  执行模型

如前所述，IC 中的复制状态机可以执行任意程序。 IC 中的基本计算单元称为 **canister** ，它与进程的概念大致相同，因为它包含程序及其状态（随时间变化）。

Canister 程序以 **WebAssembly** 或简称 **Wasm** 编码，这是一种用于基于堆栈的虚拟机的二进制指令格式。 Wasm 是一个开放标准。虽然它最初设计是让网页上支持运行高性能应用程序，但它实际上非常适合通用计算。

IC 提供了一个运行时环境，用于在canister中执行 Wasm 程序，并与其他canisters和外部用户进行通信（通过消息传递）。虽然原则上可以用任何可以编译为 Wasm 的语言编写canister程序，但还是有一种称为 **Motoko** 的语言被设计出来了，它与 IC 的操作语义非常一致。 Motoko 是一种强类型、**基于参与者(actor based)** 的编程语言，内置支持 **正交持久化(orthogonal persistence)** 和 **异步消息传递** 。正交持久化是指canister维护的所有内存都会自动持久化（即，不必将其写入文件）。 Motoko 具有许多生产力和安全特性，包括自动内存管理、泛型、类型推断、模式匹配以及任意精度和固定精度算术。

除了 Motoko，IC 还提供了一种消息接口定义语言和连接格式，叫做 **Candid** ，用于类型化的、高级的和跨语言的互操作性。这允许任何两个canisters，即使是用不同的高级语言编写的，也可以轻松地相互通信。

为了完全支持用任何特定的编程语言进行canister开发，除了该语言的 Wasm 编译器之外，还必须提供某些运行时的支持。目前，除了Motoko，IC还全面支持Rust编程语言用于canister开发。

8.  实用型代币 (Utility token)

IC 使用称为 **ICP** 的 **实用型代币** 。此令牌用于以下功能：

在 NNS 中质押：如第 1.5 节所述，ICP 代币可以在 NNS 中质押以获得投票权，从而参与控制 IC 网络的 DAO。持有 ICP 代币在 NNS 中质押并参与的用户

> NNS 治理还获得新铸造的 ICP 代币作为*投票奖励*。奖励金额由 NNS 制定和执行的政策决定。

**转换为Cycles：** ICP用于支付IC的使用费用。更具体地说，ICP 代币可以转换为 **cycles**（即*烧毁*），这些cycles用于支付创建canister（参见第 1.7 节）和canister使用的资源（存储、CPU 和带宽）的费用。 ICP 转换为cycles的汇率由 NNS 决定。

**向节点提供者支付费用：** ICP 代币用于向节点提供者支付费用 --- 这些实体拥有和运营计算节点来托管构成 IC 的副本。 NNS 定期（目前每月一次）决定每个节点提供者应该接收新铸造代币的数量，并将代币发送到节点提供者的账户。根据 NNS 制定和执行的相关政策，代币支付的条件是向 IC 提供了可靠的服务。

9.  **边界节点**

**边界节点** 提供IC的网络边缘服务。特别是，他们提供

-   明确定义的 IC 入口点，

-   IC 的拒绝服务保护，

-   从传统客户端（例如，网络浏览器）无缝访问 IC。

为了使用传统客户端无缝访问 IC，边界节点提供了将来自用户的标准 HTTPS 请求转换为指向 IC 上canister的入口消息的功能，然后将此入口消息路由到子网上存在这个canister的特定副本。此外，边界节点提供额外的服务来改善用户体验：缓存、负载平衡、速率限制，以及为传统客户端验证来自 IC 的响应。

Canister由 ic0.app 域上的 URL 标识。最初，传统客户端会查找 URL 对应的 DNS 记录，获取边界节点的 IP 地址，然后向该地址发送初始 HTTPS 请求。边界节点返回一个将在传统客户端中执行的基于 javascript 的“service worker”。在此之后，传统客户端和边界节点之间的所有交互都将通过这个 service worker 完成。

Service worker执行的一项基本任务是使用链密钥加密技术验证来自 IC 的响应（参见第 1.6 节）。为此，NNS 的公共验证密钥被硬编码在 service worker 中。

边界节点本身负责将请求路由到托管指定canister的子网上的副本。执行此路由所需的信息由边界节点从 NNS 获得。边界节点保存一个包含了能及时回复的副本的列表，并从该列表中选择一个随机副本。

传统客户端和边界节点之间以及边界节点和副本之间的所有通信都通过 TLS 保护。

除了传统客户端，还可以使用“IC原生”客户端与边界节点交互，这些客户端已经包含了service worker逻辑，不需要从边界节点检索service worker程序。

就像副本一样，边界节点的部署和配置由 NNS 控制。

10. NNS 的更多细节

如第 1.5 节所述，网络神经系统（NNS）是一种控制 IC 的算法治理系统。它是通过一个特殊系统子网上的一组canisters来实现的。此子网与任何其他子网类似，但配置有所不同（例如，系统子网上的容器不为其使用的周期收费）。一些NNS最相关的canisters是

-   **注册表canister** ，它存储 IC 的配置，即哪些副本属于哪个子网、与子网和各个副本关联的公钥等。

-   **治理canister** ，管理有关 IC 应如何发展的决策和投票。

-   **账本canister** ，用于跟踪用户的 ICP 实用型代币账户和他们之间的交易。

1.  NNS 的决策

任何人都可以通过在所谓的 **神经元** 中质押 ICP 代币来参与 NNS 治理。然后，神经元持有者可以对 **提案** 提出建议和投票，这些提案是关于应该如何更改 IC 的建议，例如，应该如何更改子网拓扑结构或协议。神经元对决策的投票权是基于权益证明。直观地说，拥有更多质押 ICP 代币的神经元拥有更多的投票权。然而，投票权还取决于其他一些神经元特征，例如，更多的投票权被赋予给承诺将其代币质押更长时间的神经元持有者。

每个提案都有一个确定的投票期。如果在投票期结束时，总投票权的简单多数投票赞成该提案，并且这些赞成票构成总投票权的给定法定人数（目前为 3%），则该提案 **被采纳** 。否则，该提议 **被拒绝** 。此外，如果绝对多数（超过总投票权的一半）赞成或反对该提案，则该提案会被立即通过或拒绝。

如果提案被采纳，治理canister会自动执行决策。例如，如果提案建议更改网络拓扑结构并被采纳，则治理canister会自动使用新配置更新注册表。

![image](https://user-images.githubusercontent.com/98205496/150631058-dcba9bc5-3bf8-47c1-a11d-d4d87428525b.png)

图 1：互联网计算机协议的层级

11. Work in progress

The architecture of the IC is still evolving and expanding. Here are a
few new features that will be deployed soon:

**DAO-controlled canisters.** Just like the overall configuration of the
IC is controlled by the NNS, any canister may also be controlled by its
own DAO, called the **service nervous system (SNS)**. The DAO
controlling a canister can control updates to the canister logic, as
well as issuing privileged commands to be carried out by the canister.

**Threshold ECDSA.** ECDSA signatures [\[JMV01\]](\l) are used in
cryptocurrencies, such as Bitcoin and Ethereum, as well as in many other
applications. While threshold signa­tures are already an essential
ingredient in the IC, these are not threshold ECDSA signatures. This new
feature will allow individual canisters to control an ECDSA sign­ing
key, which is securely distributed among all of the replicas on the
subnet hosting the canister.

**Bitcoin and Ethereum integration.** Building on the new threshold
ECDSA feature, this feature will allow canisters to interact with the
Bitcoin and Ethereum blockchains, including the ability to sign
transactions.

**HTTP integration.** This feature will allow canisters to read
arbitrary web pages (external to the IC).

2 Architecture overview

As illustrated in [Figure 1](\l), the Internet Computer Protocol
consists of four layers:

-   peer-to-peer layer (see [Section 4)](\l);

-   consensus layer (see [Section 5)](\l);

• routing layer (see [Section 6)](\l);

-   execution layer (see [Section 7)](\l).

Chain-key cryptography impacts several layers, and is discussed in
detail in [Sections 3](\l) (threshold signatures) and [8](\l)
(chain-evolution technology).

1.  Peer-to-peer layer

The peer-to-peer layer\'s task is to transport protocol messages between
the replicas in a subnet. These protocol messages consist of

-   messages used to implement consensus,

-   input messages generated by an external user.

Basically, the service provided by the peer-to-peer is a "best effort"
broadcast channel:

> *if an honest replica broadcasts a message, then that message will
> eventually be received by all honest replicas in the subnet.*

Design goals include the following:

-   **Bounded resources.** All algorithms must work with bounded
    > resources (memory, bandwidth, CPU).

-   **Prioritization.** Different messages may be treated with different
    > priorities, depend­ing on certain attributes (e.g., type, size,
    > round), and these priorities may change over time.

-   **Efficiency.** High throughput is more important than low latency.

-   **DOS/SPAM resilience.** Corrupt replicas should not prevent honest
    > replicas from communicating with one another.

2.  Consensus layer

The job of the consensus layer of the IC is to order inputs so that all
replicas in a subnet will process such inputs in the same order. There
are many protocols in the literature for this problem. The IC uses a new
consensus protocol, which is described here at a high level.

Any secure consensus protocol should guarantee two properties, which
(roughly stated) are:

-   **safety**: all replicas in fact agree on the same ordering of
    inputs, and

-   **liveness**: all replicas should make steady progress.

The IC consensus protocol is designed to be

-   extremely simple, and

-   robust: performance degrades gracefully when some replicas are
    malicious.

As discussed above, we assume *f \< n/3* faulty (i.e., Byzantine)
replicas. Also, liveness holds under a partial synchrony assumption,
while safety is guaranteed, even in a completely asynchronous network.

Like a number of consensus protocols, the IC consensus protocol is based
on a blockchain. As the protocol progresses, a tree of blocks is grown,
starting from a special **genesis block** that is the root of the tree.
Each non-genesis block in the tree contains (among other things) a
**payload**, consisting of a sequence of inputs, and a hash of the
block\'s parent in the tree. The honest replicas have a consistent view
of this tree: while each replica may have a different, partial view of
this tree, all the replicas have a view of the *same* tree. In addition,
as the protocol progresses, there is always a path of **finalized**
blocks in this tree. Again, the honest replicas have a consistent view
of this path: while each replica may have a different, partial view of
this path, all the replicas have a view of the *same* path. The inputs
in the payloads of the blocks along this path are the ordered inputs
will be processed by the execution layer of the Internet Computer.

The protocol proceeds in **rounds**. In round *h* of the protocol, one
or more blocks of **height** h are added to the tree. That is, the
blocks added in round h are always at a distance of exactly *h* from the
root. In each round, a pseudo-random process is used to assign each
replica a unique **rank**, which is an integer in the range 0,\...,n
--- 1. This pseudo-random process is implemented using a **Random
Beacon** (this makes use of threshold signatures, mentioned above in
[Section 1.6.1](\l) and discussed in more detail in [Section 3)](\l).
The replica of lowest rank is the **leader** of that round. When the
leader is honest and the network is synchronous, the leader will propose
a block, which will be added to the tree; moreover, this will be the
*only* block added to the tree in this round and it will extend the
finalized path. If the leader is not honest or the network is not
synchronous, some other replicas of higher rank may also propose blocks,
and also have their blocks added to the tree. In any case, the logic of
the protocol gives highest priority to the leader\'s proposed block and
*some* block or blocks will be added to this tree in this round. Even if
the protocol proceeds for a few rounds without extending the finalized
path, the height of the tree will continue to grow with each round, so
that when the finalized path is extended in round h, the finalized path
will be of length h. A consequence of this, even if the *latency*
occasionally increases because of faulty replicas or unexpectedly high
network latency, the *throughput* of the protocol remains essentially
constant.

The consensus protocol relies on digital signatures to authenticate
messages sent between replicas. To implement the protocol, each replica
is associated with a public verification key for a signature scheme. The
association of replicas to public keys is obtained from the registry
maintained by the NNS.

2.3 Message routing

As discussed in [Section 1.7](\l), basic computational unit in the IC is
called a **canister**. The IC provides a run-time environment for
executing programs in a canister, and to communicate with other
canisters and external users (via message passing).

The consensus layer bundles inputs into **payloads**, which get placed
into **blocks**, and as blocks are finalized, the corresponding payloads
are delivered to the **message routing** layer, then processed by the
**execution environment**, which updates the state of the canisters on
the replicated state machine and generates outputs, and these outputs
are processed by the **message routing** layer.

It is useful to distinguish between two types of inputs:

**ingress messages:** these are messages from external users;

**cross-subnet messages:** these are messages from canisters on other
subnets.

We can also distinguish between two types of outputs:

**ingress message *responses****:* these are responses to ingress
messages (which may be re­trieved by external users);

**cross-subnet messages:** these are messages to canisters on other
subnets.

Upon receiving a payload from consensus, the inputs in that payload are
placed into various **input queues**. For each canister *C* running on a
subnet, there are several in­put queues: one for ingress messages to C,
and for each other canister *C* with whom C communicates, one for
cross-subnet messages to C from *CL*

In a each round, the execution layer will consume some of the inputs in
these queues, update the replicated state of the relevant canisters, and
place outputs in various **output queues**. For each canister C running
on a subnet, there are several output queues: for each other canister
C*^f^* with whom C communicates, one for cross-subnet messages to C^f^
from C. The message routing layer will take the messages in these output
queues and place them into **subnet-to-subnet streams** to be processed
by an **crossnet transfer protocol**, whose job it is to actually
transport these messages to other subnets.

In addition to these output queues, there is also an **ingress history**
data structure. Once an ingress message has been processed by a
canister, a **response** to that ingress message will be recorded in
this data structure. At that point, the external user who provided the
ingress message will be able to retrieve the corresponding response.
(Note that *ingress history* does not maintain the full history of all
ingress messages.)

Note that the replicated state comprises the state of the canisters, as
well as "system state", including the above-mentioned queues and
streams, as well as the *ingress history* data structure. Thus, both the
message routing and execution layers are involved in updating and
maintaining the replicated state of a subnet. It is essential that all
of this state is updated in a completely *deterministic* fashion, so
that all replicas maintain *exactly* the same state.

Also note that the consensus layer is decoupled from the message routing
and execution layers, in the sense that any forks in the consensus
blockchain are resolved before their payloads are passed to message
routing, and in fact, consensus does not have to keep in lock step with
message routing and consensus and is allowed to run a bit ahead.

2.3.1 Per-round certified state

In each round, *some* of the state of a subnet will be *certified.* The
**per-round certified state** is certified using chain-key cryptography.
Among other things, the certified state in a given round consists of

-   *cross-subnet messages* that were recently added to the
    subnet-to-subnet streams;

-   other metadata, including the *ingress history* data structure.

The **per-round certified state** is certified using a threshold
signature (see [Section 1.6.1)](\l). Per-round certified state is used
in several ways in the IC:

-   *Output authentication.* Cross-subnet messages and responses to
    > ingress messages are authenticated using per-round certified
    > state.

-   *Preventing and detecting non-determinism.* Consensus guarantees
    > that each replica processes inputs in the same order. Since each
    > replica processes these inputs determin­istically, each replica
    > should obtain the same state. However, the IC is designed with an
    > extra layer of robustness to prevent and detect any (accidental)
    > non-deterministic computation, should it arise. The per-round
    > certified state is one of the mechanisms used to do this.

-   *Coordination with consensus.* The per-round certified state is also
    > used to coordinate the execution and consensus layers, in two
    > different ways:

> **-**If consensus is running ahead of execution (whose progress is
> determined by the last round whose state is certified), consensus will
> be "throttled".
>
> **-** Inputs to consensus must pass certain validity checks, and these
> validity checks may depend on certified state, which all replicas must
> agree upon.

2.  Query calls vs update calls

As we have described it so far, an ingress messages must pass through
consensus so that they are processed in the same order by all replicas
on a subnet. However, an important optimization is available to those
ingress messages whose processing does not modify the replicated state
of a subnet. These are called **query calls** --- as opposed to other
ingress messages, which are called **update calls**. Query calls are
allowed to perform computations which read and possibly update the state
of a canister, but any updates to the state of a canister are never
committed to the replicated state. As such, a query call may be
processed directly by a single replica without passing through
consensus, which greatly reduces the latency for obtaining a response
from a query call.

In general, a response to a query call is not recorded in the *ingress
history* data structure, and therefore cannot be authenticated using the
per-round certified state mechanism as described above. However, the IC
makes it possible for canisters to store data (while processing update
calls) in special *certified variables,* which *can* be authenticated by
this mechanism; as such, query calls that return as their value a
certified variable can still be authenticated.

3.  External user authentication

One of the main differences between an ingress message and a
cross-subnet message is the mechanism used for authenticating these
messages. While chain-key cryptography is used to authenticate
cross-subnet messages, a different mechanism is used to authenticate
ingress messages from external users.

There is no central registry for external users. Rather, an external
user identifies himself to a canister using a **user identifier**, which
is a hash of a public signature-verification key. The user holds a
corresponding secret signing key, which is used to sign ingress
messages. Such a signature, as well as the corresponding public key, is
sent along with the ingress message. The IC automatically authenticates
the signature and passes the user identifier to the appropriate
canister. The canister may then authorize the requested operation, based
on the user identifier and other parameters to the operation specified
in the ingress message.

First-time users generate a key pair and derive their user identifier
from the public key during their first interaction with the IC.
Returning users are authenticated using the secret key that is stored by
the user agent. A user may associate several key pairs with a single
user identity, using signature delegation. This is useful, as it allows
a single user to access the IC from several devices using the same user
identity.

4.  Execution layer

The execution layer processes one input at a time. This input is taken
from one of the input queues, and is directed to one canister. Based on
this input and the state of the canister, the execution environment
updates the state of the canister, and additionally may add messages to
output queues and update the *ingress history* (possibly with a response
to an earlier ingress message).

Each subnet has access to a **distributed pseudorandom generator
(PRG)**. Pseudo­random bits are derived from a seed that itself is a
threshold signature called the **Random Tape** (see [Section 1.6.1](\l)
and more detail in [Section 3)](\l). There is a different Random Tape
for each round of the consensus protocol.

The basic properties of the random tape are:

1.  Before a block at height *h* is finalized by any honest replica, the
    > Random Tape at height h + 1 is guaranteed to be unpredictable.

2.  By the time block at height h + 1 is finalized by any honest
    > replica, that replica will typically have all the shares it needs
    > to construct the Random Tape at height h + 1.

To obtain pseudorandom bits, a subnet must make a request for these bits
via a "system call" from the execution layer in some round, say h. The
system will then respond to that request later, using the Random Tape at
height h+1. By property (1) above, it is guaranteed that the requested
pseudorandom bits are unpredictable at the time the request is made. By
property (2) above, the requested random bits will typically be
available at the time the next block is finalized.

5.  Putting it all together

We trace through the typical flow to process a user request on the IC.

> 1\. A user\'s request *M* to a canister *C* is sent by the user\'s
> client to a boundary node (see [Section 1.9)](\l), and the boundary
> node sends M to a replica on the subnet that hosts canister C.

2.  After receiving *M*, this replica will broadcast *M* to all other
    > replicas on the subnet, using the peer-to-peer layer (see [Section
    > 2.1)](\l).

3.  Having received M, the leader for the next round of consensus (see
    > [Section 2.2)](\l) will bundle M with other inputs to form the
    > payload for a block *B* that the leader proposes.

4.  Some time later, block B is finalized and the payload is sent to the
    > message routing layer (see [Section 2.3)](\l) for processing. Note
    > that the peer-to-peer layer is also used by consensus to finalize
    > this block.

5.  The message routing layer will place M in the input queue of the
    canister *C*.

6.  Some time later, the execution layer (see [Section 2.4)](\l) will
    > process M, updating the internal state of the canister C.

> In some situations, the canister C will be able to immediately compute
> a response *R* to the request M. In this case, R is placed in the
> ingress history data structure.
>
> In other situations, processing the request M may require making a
> request to a another canister. In this example, let us suppose that to
> process this particular request M, the canister C must make a request
> M^f^ to another canister *C* that resides on another subnet. This
> second request M^1^ will be placed in the output queue of the C, and
> then the following steps are performed.

7.  Some time later, message routing will move M^f^ into an appropriate
    > cross-subnet stream, and this will eventually be transported to
    > the subnet hosting C*^1^*.

8.  On the second subnet, the request M^f^ will be obtained from the
    > first subnet, and eventually pass through consensus and message
    > routing on the second subnet and then be processed by execution.
    > The execution layer will update the internal state of canister
    > C*^1^* and generate a response *R!* to the request M^1^. The
    > response *R* will go in the output queue of canister Cand
    > eventually be placed in a cross-subnet stream and transported back
    > to the first subnet.

9.  Back on the first subnet, the response *R* will be obtained from the
    > second subnet, and eventually pass through consensus and message
    > routing on the first subnet and then be processed by execution.
    > The execution layer will update the internal state of canister C
    > and generate a response R to the original request message M. This
    > response will be recorded in the ingress history data structure.

Regardless of which execution path is taken, the response R to request M
will eventually be recorded in the ingress history data structure on the
subnet that hosts canister C. To obtain this response, the user\'s
client must perform a kind of "query call" (see [Section 2.3.2)](\l). As
discussed in [Section 2.3.1](\l), this response will be authenticated
via chain-key cryptography (specifically, using a threshold signature).
The authentication logic itself (i.e., threshold signature verification)
will be performed by the client using the service worker originally
obtained by the client from the boundary node.

3 Chain-key crytography I: threshold signatures

A critical component of the IC\'s chain-key cryptography is a threshold
signature scheme [\[Des87\]](\l). The IC uses threshold signatures for a
number of purposes. Let *n* be the number of replicas in a subnet and
let *f* be a bound on the number of corrupt replicas.

-   The Consensus Layer makes use of an (f + 1)-out-of-n threshold
    > signature to realize a *random beacon* (see [Section 5.5)](\l).

-   The Execution Layer makes use of an (f + 1)-out-of-n threshold
    > signature to realize a *random tape,* which is used to provide
    > unpredictable pseudorandom numbers to canisters (see [Section
    > 7.1)](\l).

-   The Execution Layer makes use of an (n --- f )-out-of-n threshold
    > signature to *certify the replicated state.* This is used both to
    > authenticate the outputs of a subnet (see [Section 6.1)](\l) and
    > to implement the *fast-forwarding* feature of the IC\'s
    > *chain-evolution technology* (see [Section 8.2)](\l).

For the first two applications (the random beacon and random tape), it
is essential that the threshold signatures are *unique,* i.e., for a
given public key and message, there is only one valid signature. This is
required as we use the signature as a seed to a pseudorandom generator,
and all replicas who compute such a threshold signature must agree on
the same seed.

1.  Threshold BLS signatures

We implement threshold signatures based on the BLS signature scheme
[\[BLS01\]](\l), which is trivial to adapt to the threshold setting.

The ordinary (i.e., non-threshold) BLS signature scheme makes use of two
groups, *G* and *G,* both of prime order *q.* We assume that G is
generated by *g* G G and G' is generated by *g* G G'. We also assume a
hash function *[Hg^!^]{.smallcaps}* that maps its inputs to G' (and
which is modeled as a random oracle). The secret signing key is an
element *x* G *Zq* and the public verification key is *V* := *g^x^* G G.

In the non-threshold setting, to sign a message m, the signer computes
*h^f^ --- [Hg^!^]{.smallcaps}* (m) G G^z^ and then computes the
signature *a* := (h/)^x^ G G^z^. To verify that such a signature is
valid, one must test if log# *a* = logg V. To be able to perform this
test efficiently, the BLS scheme uses the notion of a **pairing** on the
groups G and G'，which is a special algebraic tool that is available
when G and *G^f^* are **elliptic curves** of a special type. We shall
not be able to go into the details of pairings and elliptic curves here.
See [\[BLS01\]](\l) for more details. BLS signatures have the nice
property (mentioned above) that signatures are unique.

In the t-out-of-n threshold setting, we have n replicas, any t of which
may be used to generate a signature on a message. On somewhat more
detail, each replica *Pj* holds a share Xj G *Zq* of the secret signing
key x G *Zq*, which is privately held by *Pj*, while the group element
*Vj* := *g^xj^* is publicly available. The shares (xi,\...,x~n~) are a
t-out-of-n secret-sharing of x (see [Section 3.4)](\l).

Given a message m, replica *Pj* can generate a **signature share**

aj := (h\')^x^J G G'，

where *h^f^* ：= *H®* (m) as before. To verify that such a signature
share is valid, one must test if log# *Oj* = logg *Vj*. This can be done
using a pairing, as discussed above --- in fact, this is exactly the
same as the validity test for an ordinary BLS signature with public key
Vj-.

This scheme satisfies the following **reconstruction property**:

> *Given any collection of* t *valid signature shares Oj on a message m
> (contributed by distinct replicas), we can efficiently compute a valid
> BLS signature* [o]{.smallcaps} *on m under the public verification
> key.*

*In fact,* [o]{.smallcaps} *can be computed as*

![](./images/media/image2.png){width="0.5069444444444444in"
height="0.34652777777777777in"}

> *where the Xj\'s can be efficiently computed just from the indices of
> the* t *con­tributing replicas.*

Under reasonable intractability assumptions for G, and modeling *H®* as
a random oracle, this scheme satisfies the following **security
property**:

> *Assume that at most f replicas may be corrupted by an adversary. Then
> it is infeasible for the adversary to compute a valid signature on a
> message unless it obtains signature shares on that message from at
> least* t --- f *honest replicas.*

2.  Distributed key distribution

To implement threshold BLS, we need a way to distribute the shares of
the secret signing key to the replicas. One way to do this would be to
have a **trusted party** compute all of these shares directly and
distribute them to all the replicas. Unfortunately, this would create a
single point of failure. Instead, we use a **distributed key generation
(DKG) protocol**, which allows the replicas to essentially carry out the
logic of such a trusted party using a secure distributed protocol.

We sketch the high level ideas of the protocol currently implemented. We
refer the reader to [\[Gro21\]](\l) for more details. The DKG protocol
used is essentially non-interactive. It uses two essential ingredients:

-   a **publicly verifiable secret sharing (PVSS)** scheme, and

-   a **consensus** protocol.

Although any consensus protocol could be used, not surprisingly, the one
we use is that in [Section 5](\l) (see also [Section 8)](\l).

3.  Assumptions

The basic assumptions made are the same as outlined in [Section 1:](\l)

-   asynchronous communication, and

-   f \< n/3.

We only indirectly make use of a *partial* synchrony assumption (as in
[Section 5.1)](\l) to ensure that the consensus protocol attains
liveness.

We also assume that for a t-out-of-n threshold signature scheme, we have

*f \<* t \< n - *f,*

which (among other things) ensures that (1) the corrupt replicas cannot
sign all by them­selves, and (2) the honest replicas can sign all by
themselves.

We also assume that every replica is associated with some public keys,
where each replica also holds the corresponding private key. One public
key is the signing key (the same one as in [Section 5.4)](\l). Another
public key is a public encryption key for a specific public-key
encryption scheme needed to implement the PVSS scheme (details follow).

4.  PVSS scheme

Let *G* be the group of prime order *q* generated by *g* G *G*
introduced above. Let s G *Zq* be a secret. Recall that a t-out-of-n
Shamir secret-sharing of s is a vector (si, ) G

where

*Sj* ：= *a(j) (j* = 1,\...,n).

and

a(x) := *ao* + *aix* + - - - + *at-ix^t-1^* G Zq\[x\]

is a polynomial of degree less than t with ao := s. The key properties
of such a secret sharing are

-   from any collection of t of the sj\'s, we can efficiently compute
    > (via polynomial inter­polation) the secret s = ao = a(0), and

-   if ai,\..., *at-i* are chosen uniformly and independently over Zq,
    > then any collection of fewer than t of the sj\'s reveals no
    > information about the secret s.

At a high level, a PVSS scheme allows one replica, *Pi*, called the
**dealer**, to take such a sharing, and compute an object called a
**dealing**, which contains

-   a vector of group elements (Ao, At-i), where *Ak* := *g^ak^* for *k*
    = 0,\...,t --- 1,

-   a vector of ciphertexts (ci,c~n~), where *Cj* is the encryption of
    > sj under *Pj*\'s public encryption key,

-   a non-interactive zero-knowledge proof n that each Cj does indeed
    > encrypt such a share --- more precisely, that each Cj decrypts the
    > value sj satisfying

t-i

*g^s^j* = n *Ak* = g^a(j)^.

k=0

We note that to establish the overall security of our DKG protocol, the
PVSS scheme must provide an appropriate level of chosen ciphertext
security. Specifically, the dealer must embed its identity as
*associated data* in the dealing, and the encrypted shares must

remain hidden, even under a chosen ciphertext attack wherein an
adversary is allowed to decrypt arbitrary dealings which are decrypted
under associated data that is distinct from the associated data used to
create the dealing.

It is easy to realize a PVSS scheme, if one is not too concerned about
efficiency. The idea is to use an ElGamal-like encryption scheme to
encrypt each *Sj* bit by bit, and then use a standard non-interactive
zero-knowledge proof for the relation [(2)](\l), which would be based on
an a standard application of the Fiat-Shamir transform (see
[\[FS86\]](\l)) to an appropriate Sigma protocol (see [\[CDS94\]](\l)).
While this yields a polynomial-time scheme, it is not that practical.
However, there are many possible ways to optimize this type of scheme.
See [\[Gro21\]](\l) for the details on the highly optimized PVSS scheme
used in the IC.

3.5 The basic DKG protocol

Using the PVSS scheme and a consensus protocol, the basic DKG protocol
is very simple.

1.  Each replica broadcasts a **signed dealing** of a random secret to
    all other replicas.

> Such a signed dealing includes a dealing, along with the identity of
> the dealer and a signature on the dealing under the dealer\'s public
> signing key.
>
> Such a signed dealing is called **valid** if it has the right
> syntactic form, and the signature and non-interactive zero knowledge
> proof are valid.

2.  Using consensus the replicas agree on a set *S* of *f* + 1 valid
    > signed dealings (from distinct dealers).

3.  Suppose that the ith dealing in the set S contains the vector of
    > group elements (Ai,o,*Ai,t-i)* and the vector of ciphertexts
    > (ci,i,

Then the public verification key for the threshold signature scheme is

Ai,0 -

Note that the secret signing key is implicitly defined as

*X* := logg V.

*Pj*\'s share of the secret signing key x is

^x^j ^:^=

> where s《,j is the decryption of *Ci,j* under *Pj*\'s secret
> decryption key. The public verification key for replica *P*~j~ is

t-i

nn *j*

i k=0

Note that the shares *Xj* comprise a t-out-of-n Shamir secret-sharing of
*x.* As such, the *Xj* values appearing in [(1)](\l) are just Lagrange
interpolation coefficients. This establishes the *reconstruction
property* stated in [Section 3.1](\l). As for the security property
stated in [Section 3.1](\l), this can be proved to hold modeling
*[H(g^/^]{.smallcaps}* as a random oracle, and assuming that the PVSS
scheme is secure, and that the groups *G* and (with a pairing) satisfy a
certain type of **one-more Diffie-Hellman** hardness assumption, which
can be stated as saying that no efficient adversary can win the
following game with non-negligible probability:

*The challenger chooses 卩顷...,卩k* € *Z~q~ and* € *Z~q~ at random, and
gives*

*{g 网*}Li *and* {(g\')^Vj^ }；=i *to the adversary.*

> *The adversary makes a sequence of queries to the challenger, each of
> which is a vector of the form* {K《,j}《,j*, to which the challenger
> responds with*

H((g\"广。.

i,j

> *To end the game, the adversary outputs a vector* {X《,j}《,j *and a
> group element h!* € G'，*and wins the game if*

![](./images/media/image3.png){width="1.3666666666666667in"
height="0.4131944444444444in"}

> *and the output vector* {X《,j}《,j *is not a linear combination of
> the query vectors.*

While this type of one-more Diffie-Hellman assumption is needed in the
case where *t \> f* + 1, one can get by with a weaker assumption when t
= *f* + 1 (the so-called co-CDH assumption, on which the security of the
ordinary BLS scheme is based).

3.6 A resharing protocol

The basic DKG protocol can be easily modified so that instead of
creating a sharing of a fresh random secret *x*, it instead creates a
fresh, random sharing of a previously shared secret.

-   Step 1 of the basic protocol is modified so that each replica
    > broadcasts a signed dealing of its existing share.

-   Step 2 is modified so that a set of t valid signed dealings is
    > agreed upon. Also, each

> dealing is verified to ensure that it is indeed a dealing of the
> appropriate existing share (this means that the value of in the ith
> dealing should be equal to the old value oE). \'

-   In Step 3, the computation of the new Xj (and *Vj*) values weight
    > the sum (and product) on *i* Lagrange interpolation coefficients.

4 Peer-to-peer layer

The peer-to-peer layer\'s task is to transport protocol messages between
the replicas in a subnet. These protocol messages consist of

-   messages used to implement consensus, e.g., block proposals,
    > notarizations, etc. (see [Section 5)](\l);

-   ingress messages (see [Section 6)](\l).

Basically, the service provided by the peer-to-peer is a "best effort"
broadcast channel: *if an honest replica broadcasts a messages, then
that message will eventually be received by all honest replicas in the
subnet.*

Design goals include the following:

-   **Bounded resources.** All algorithms must work with bounded
    > resources (memory, bandwidth, CPU).

-   **Prioritization.** Different messages may be treated with different
    > priorities, depend­ing on certain attributes (e.g., type, size,
    > round), and these priorities may change over time.

-   **Efficiency.** High throughput is more important than low latency.

-   **DOS/SPAM resilience.** Corrupt replicas should prevent honest
    > replicas from com­municating with one another.

Observe that in the consensus protocol, some messages, notably block
proposals (which can be quite large), will be rebroadcast by all
replicas. This is necessary to ensure correct behavior of that protocol.
However, if implemented naively, this would be a huge waste of
resources. To avoid having all replicas broadcasting the same message,
the peer-to-peer layer makes use of an **advertise-request-deliver**
mechanism. Instead of broadcasting a (large) message directly, it will
instead broadcast a (small) **advertisement** for the message: if a
replica receives such an advertisement, has not already received, and
deems the message to be important, it will **request** that the message
is **delivered**. This strategy decreases bandwidth utilization at the
cost of higher latency. For small messages, this trade-off is not
worthwhile, and it makes more sense to just send the message directly,
rather than an advertisement.

For relatively small subnets, a replica that wishes to broadcast a
message will send an advertisement to all replicas in the subnet, each
of which may then request that the message is delivered. For larger
subnets, this advertise-request-deliver mechanism may operate over an
**overlay network**. An overlay network is a connected, undirected graph
whose vertices comprise the replicas in a subnet. Two replicas are
**peers** if there is an edge connecting them in this graph, and replica
only communicates with its peers. So when a replica wishes to broadcast
a message, it sends an advertisement for that message to its peers.
Those peers may request that the message be delivered, and upon
receiving the message, if certain conditions are met, those peers will
advertise the message to their peers. This is essentially a **gossip
network**. This strategy again decreases bandwidth utilization at the
cost of even higher latency.

5 Consensus Layer

The job of the consensus layer of the IC is to order inputs so that all
replicas in a subnet will process such inputs in the same order. There
are many protocols in the literature for this problem. The IC uses a new
consensus protocol, which is described here at a high level. For more
details, see the paper [\[CDH+21\]](\l) (in particular, Protocol ICC1 in
that paper).

Any secure consensus protocol should guarantee two properties, which
(roughly stated) are:

-   **safety**: all replicas in fact agree on the same ordering of
    inputs, and

-   **liveness**: all replicas should make steady progress.

The paper [\[CDH+21\]](\l) proves that the IC consensus protocol
satisfies both of these prop­erties

The IC consensus protocol is designed to be

-   extremely simple, and

-   robust: performance degrades gracefully when some replicas are
    malicious.

1.  Assumptions

As discussed in the introduction, we assume

-   a subnet of *n* replicas, and

-   at most *f \<* n/3 of the replicas are faulty.

Faulty replicas may exhibit arbitrary, malicious (i.e., Byzantine)
behavior.

We assume that communication is **asynchronous**, with no *a priori*
bound on the delay of messages sent between replicas. In fact, the
scheduling of message delivery may be completely under adversarial
control. The IC consensus protocol guarantees safety under this very
weak communication assumption. However, to guarantee liveness, we need
to assume a form of **partial synchrony**, which (roughly stated) says
that the network will be periodically synchronous for short intervals of
time. In such intervals of synchrony, all undelivered messages will be
delivered in less than time 5, for some fixed bound *5.* The bound 5
does not have to be known in advance (the protocol is initialized with a
reasonable bound, but will dynamically adapt and increase this bound if
it is too small). Regardless of whether we are assuming an asynchronous
or a partially synchronous network, we assume that every message sent
from one honest replica to another will *eventually* be delivered.

2.  Protocol overview

Like a number of consensus protocols, the IC consensus protocol is based
on a blockchain. As the protocol progresses, a tree of blocks is grown,
starting from a special **genesis block** that is the root of the tree.
Each non-genesis block in the tree contains (among other things) a
**payload**, consisting of a sequence of inputs, and a hash of the
block\'s parent in the tree. The honest replicas have a consistent view
of this tree: while each replica may

have a different, partial view of this tree, all the replicas have a
view of the *same* tree. In addition, as the protocol progresses, there
is always a path of **finalized** blocks in this tree. Again, the honest
replicas have a consistent view of this path: while each replica may
have a different, partial view of this path, all the replicas have a
view of the *same* path. The inputs in the payloads of the blocks along
this path are the ordered inputs will be processed by the execution
layer of the Internet Computer (see [Section 7)](\l).

The protocol proceeds in **rounds**. In round *h* of the protocol, one
or more blocks of **height** h are added to the tree. That is, the
blocks added in round h are always at a distance of exactly h from the
root. In each round, a pseudo-random process is used to assign each
replica a unique **rank**, which is an integer in the range --- 1. This

pseudo-random process is implemented using a **random beacon** (see
[Section 5.5](\l) below). The replica of lowest rank is the **leader**
of that round. When the leader is honest and the network is synchronous,
the leader will propose a block, which will be added to the tree;
moreover, this will be the *only* block added to the tree in this round
and it will extend the finalized path. If the leader is not honest or
the network is not synchronous, some other replicas of higher rank may
also propose blocks, and also have their blocks added to the tree. In
any case, the logic of the protocol gives highest priority to the
leader\'s proposed block and *some* block or blocks will be added to
this tree in this round. Even if the protocol proceeds for a few rounds
without extending the finalized path, the height of the tree will
continue to grow with each round, so that when the finalized path is
extended in round h, the finalized path will be of length h. A
consequence of this, even if the *latency* occasionally increases
because of faulty replicas or unexpectedly high network latency, the
*throughput* of the protocol remains essentially constant.

3.  **Additional properties**

An additional property enjoyed by the IC consensus protocol (just like
PBFT [\[CL99\]](\l) and HotStuff [\[AMN+20\]](\l), and unlike others,
such as Tendermint [\[BKM18\]](\l)) is *optimistic respon­siveness*
[\[PS18\]](\l), which means that when the leader is honest, the protocol
may proceed at the pace of the actual network delay, rather than some
upper bound on the network delay.

We note that the simple design of the IC consensus protocol also ensures
that its per­formance degrades quite gracefully when and if Byzantine
failures actually do occur. As pointed out in [\[CWA+09\]](\l), much of
the recent work on consensus has focused so much on improving the
performance in the "optimistic case" where there are no failures, that
the resulting protocols are dangerously fragile, and may become
practically unusable when failures do occur. For example,
[\[CWA+09\]](\l) show that the throughput of existing implemen­tations
of PBFT drops to zero under certain types of (quite simple) Byzantine
behavior. The paper [\[CWA+09\]](\l) advocates for *robust* consensus,
in which *peak* performance under optimal conditions is partially
sacrificed in order to ensure *reasonable* performance when some parties
actually are corrupt (but still assuming the network is synchronous).
The IC consesus protocols is indeed robust in the sense of
[\[CWA^+^09\]](\l): in any round where the leader is corrupt (which
itself happens with probability less than 1/3), the protocol will
effectively allow another party to take over as leader for that round,
with very little fuss, to move the protocol forward to the next round in
a timely fashion.

4.  **Public keys**

To implement the protocol, each replica is associated with a public key
for the BLS signature scheme [\[BLS01\]](\l), and each replica also
holds the corresponding secret signing key. The association of replicas
to public keys is obtained from the registry maintained by the NNS (see
[Section 1.5)](\l). These BLS signatures will be used to authenticate
messages sent by replicas.

The protocol also uses the **signature aggregation** feature ofBLS
signatures [\[BGLS03\]](\l), which allows many signatures on the same
message to be aggregated into a compact multi­signature. The protocol
will use these multi-signatures for **notarizations** (see [Section
5.7)](\l) and **finalizations** (see [Section 5.8)](\l), which are
aggregations of *n --- f* signatures on messages of a certain form.

5.  **Random Beacon**

In addition to BLS signatures and multi-signatures as discussed above,
the protocol makes use of a BLS threshold signature scheme to implement
the above-mentioned random beacon. The random beacon for height *h* is a
(f + 1)-threshold signature on a message unique to height h. In each
round of the protocol, each replica broadcasts its share of the beacon
for the next round, so that when the next round begins, all replicas
should have enough shares to reconstruct the beacon for that round. As
discussed above, the random beacon at height h is used to assign a
pseudo-random rank to each replica that will be used in round h of the
protocol. Because of the security properties of the threshold signature,
an adversary will not be able to predict the ranking of the replicas
more than one round in advance, and these rankings will effectively be
as good as random. See [Section 3](\l) for more on BLS threshold
signatures.

6.  **Block making**

Each replica may at different points in time play the role of a **block
maker**. As a block maker in round h, the replica proposes a block *B*
of height h that to be child of a block *B^f\ ^*of height h --- 1 in the
tree of blocks. To do this, the block maker first gathers together a
**payload** consisting of all inputs it knows about (but not including
those already included in payloads in blocks in the path through the
tree ending at B'). The block B consists of

-   the payload,

-   the hash of B'，

-   the rank of the block maker,

-   the height h of the block.

After forming the block B, the block maker forms a **block proposal**,
consisting of

-   the block B,

-   the block maker\'s identity, and

-   the block maker\'s signature on *B.*

A block maker will broadcast its block proposal to all other replicas.

7.  **Notarization**

A block is effectively added to the tree of blocks when it becomes
**notarized**. For a block to become notarized, *n --- f* distinct
replicas must **support** its notarization.

Given a proposed block B at height h, a replica will determine if the
proposal is **valid**, which means that B has the syntactic form
described above. In particular, B should contain the hash of a block
*B^f^* of height *h^f^* that is already in the tree of blocks (i.e.,
already notarized). In addition, the payload of B must satisfy certain
conditions (in particular, all of the inputs in the payload must satisfy
various constraints, but these constraints are generally independent of
the consensus protocol). Also, the rank of the block maker (as recorded
in the block *B*) must match the rank assigned in round *h* by the
random beacon to the replica that proposed the block (as recorded in the
block proposal) .

If the block is valid and certain other constraints hold, the replica
will **support** the notarization of the block by broadcasting a
**notarization share** for B, consisting of

-   the hash of B,

-   the height h of B,

-   the identity of the supporting replica, and

-   the supporting replica\'s signature on a message comprising the hash
    > of B and the height h.

Any set of n --- f notarization shares on B may be aggregated together
to form a **nota­rization** for B, consisting of

-   the hash of B,

-   the height h of B,

-   the set of identities of the n --- f supporting replicas,

-   an aggregation of the n --- f signatures on the message comprising
    > the hash of B and the height h.

As soon as a replica obtains a notarized block of height h, it will
finish round h, and will subsequently not support the notarization of
any other blocks at height h. At this point in time, such a replica will
also *relay this notarization to all other replicas.* Note that this
replica may have obtained the notarization either by (1) receiving it
from another replica, or (2) aggregating n --- f notarization shares
that it has received.

The **growth invariant** states that each honest replica will eventually
complete each round and start the next, so that the tree of notarized
blocks continues to grow (and this holds only assuming asynchronous
eventual delivery, and not partial synchrony). We prove the growth
invariant below (see [Section 5.11.4)](\l).

8.  **Finalization**

There may be more than one notarized block at a given height *h.*
However, if a block is **finalized**, then we can be sure that there is
no other notarized block at height h. Let us call this the **safety
invariant**.

For a block to become finalized, *n --- f* distinct replicas must
support its finalization. Recall that round h ends for a replica when it
obtains a notarized block *B* of height h. At that point in time, such a
replica will check if it supported the notarization of any block at
height h *other* than block B (it may or may not have supported the
notarization of B itself). If not, the replica will support the
finalization of B by broadcasting a **finalization share** for B. A
finalization share has exactly the same format as a notarization share
(but is tagged in such a way notarization shares and finalization shares
cannot be confused with one another). Any set of n --- f finalization
shares on B may be aggregated together to form a **finalization** for B,
which has exactly the same format as a notarization (but again, is
appropriately tagged). Any replica that obtains a finalized block will
broadcast the finalization to all other replicas.

We prove the safety invariant below (see [Section 5.11.5)](\l). One
consequence of the safety invariant is the following. Suppose two blocks
B and *B^f^* are finalized, where B has height h, *B^f^* has height
*h^f^ \< h.* Then the safety invariant implies that the path in the tree
of notarized blocks ending at B*^1^* is a prefix of the path ending at B
(if not, then there would be two notarized blocks at height *h,*
contradicting the finalization invariant). Thus, whenever a replica sees
a finalized block B, it may view all ancestors of B as being
**implicitly finalized**, and because of the safety invariant, the
safety property is guaranteed to hold for these (explicitly and
implicitly) finalized blocks --- that is, all replicas agree on the
ordering of these finalized blocks.

9.  **Delay functions**

The protocol makes use of two **delay functions**, A~m~ and A~n~, which
control the timing of block making and notarization activity. Both of
these functions map the rank *r* of the proposing replica to a
nonnegative delay amount, and it is assumed that each function is
monotonely increasing in r, and that Am(r) \< An(r) for all r = 0,\...,n
--- 1. The recommended definition of these functions is Am(r) = *25r*
and An(r) = *25r* + e, where *5* is an upper bound on the time to
deliver messages from one honest replica to another, and e \> 0 is a
\"governor" to keep the protocol from running too fast. With these
definitions, liveness will be ensured in those rounds in which (1) the
leader is honest, and (2) messages really are delivered between honest
replicas within time 5. Indeed, if (1) and (2) both hold in a given
round, then the block proposed by the leader in that round will be
finalized. Let us call this the **liveness invariant**. We prove this
below (see [Section 5.11.6)](\l).

10. **An example**

[Figure 2](\l) illustrates a block tree. Each block is labeled with its
height (30, 31, 32, \...) and

the rank of its block maker. The figure also shows that each block in
the tree is notarized, as indicated by the ® symbol. This means that for
each notarized block in the tree, at

![image](https://user-images.githubusercontent.com/98205496/150631007-13d851da-1ad3-4058-b2ca-771c071e9d3e.png)
Figure 2: An example tree of blocks

than one notarized block in the tree at a given height. For example, at
height 32, we see there are two notarized blocks, one proposed by block
makers of rank 1 and 2. The same thing happens at height 34. We can also
see that the block at height 36 is also explicitly finalized, as
indicated by the ♦ symbol. This means that *n --- f* distinct replicas
supported this block\'s finalization, which means that these replicas
(or at least, the honest replicas among these) did not support the
notarization of any other block. All of the ancestors of this block,
which are shaded gray, are considered implicitly finalized.

**5.11 Putting it all together**

We now describe in more detail how the protocol works; specifically, we
describe more precisely when a replica will propose a block and when a
replica will support the notarization of a block. A given replica *P*
will record the time at which it enters a given round h, which happens
when it has obtained (1) some notarization for a block of height *h ---*
1, and (2) the random beacon for round h. Since the random beacon for
round h has been determined, P can determine its own rank *rp*, as well
as the rank *[tq]{.smallcaps}* of each other replica *Q* for round h.

1.  Random beacon details

As soon as a replica has received the random beacon for round *h*, or
enough shares to contruct the random beacon for round h, it will *relay
the random beacom for round h to all other replicas.* As soon as a
replica enters round h, it will generate and broadcast its share of the
random beacon at round h + 1.

2.  Block making details

Replica P will only propose its own block *Bp* provided (1) at least
A~m~(rp) time units have passed since the beginning of the round, and
(2) there is no valid lower ranked block currently seen by P.

Note that since *P* is guaranteed to have a notarized block of height *h
---* 1 when it enters round h, it can make its proposed block a child of
this notarized block (or any other notarized block of height h --- 1
that it may have). Also note that when *p* broadcasts its proposal for
*Bp*, it must also ensure that it also has *relayed the notarization of
Bp\'s parent to all replicas.*

Suppose a replica *Q* sees a valid block proposal from a replica P of
rank *rp \< [tq]{.smallcaps}* such that (1) at least A~m~(rp) time units
have passed since the beginning of the round, and (2) there is no block
of rank less than rp currently seen by Q. Then at this point in time, if
it has not already done so, Q will *relay this block proposal (along
with the notarization of the proposed block\'s parent) to all other
replicas.*

3.  Notarization details

Replica P will support the notarization of a valid block
[Bq]{.smallcaps} proposed by a replica Q of rank [tq]{.smallcaps}
provided (1) at least [A~h~(tq)]{.smallcaps} time units have passed
since the beginning of the round, and (2) there is no block of rank less
than [tq]{.smallcaps} currently seen by P.

4.  Proof of growth invariant

The growth invariant states that each honest replica will eventually
complete each round and start the next. Assume that all honest replicas
have started round h. Let *r\** be the rank of the lowest ranked honest
replica P\* in round h. Eventually, P\* will either (1) propose its own
block, or (2) relay a valid block proposed by a lower ranked replica. In
either case, some block must eventually be supported by all honest
replicas, which means that some block will become notarized and all
honest replicas will finish round h. All honest replicas will also
receive the shares needed to construct the random beacon for round h +
1, and so will start round h + 1.

5.  Proof of safety invariant

The **safety invariant** states that if a block is finalized in a given
round, then no other block may be notarized in that round. Here is a
proof of the safety invariant:

1.  Suppose that the number of corrupt replicas is exactly *f* \* \< *f
    \<* n/3.

2.  If a block B is finalized, then its finalization must have been
    > supported by a set *S* of at least *n --- f --- f \** honest
    > replicas (by the security property for aggregate signatures).

3.  Suppose (by way of contradiction) that another block *B^f^* = B were
    > notarized. Then its notarization must have been supported by a set
    > *S^!^* of at least n --- f --- f \* honest replicas (again, by the
    > security property for aggregate signatures).

4.  The sets S and S\' are disjoint (by the finalization logic).

5.  Therefore, n --- *f\* \>* \|S U S\'\| = \|S\| + \|S\'\| \> 2(n --- f
    > --- f\*), which implies n \< 3f, a contradiction.

```{=html}
<!-- -->
```
6.  Proof of liveness invariant

We say that the network is \^**-synchronous at time** t if all messages
that have been sent by honest replicas at or before time t arrive at
their destinations before time t.

The **liveness invariant** may be stated as follows. Suppose that
A~n~(1) \> A~m~(0) + 25. Also suppose that in a given round h, we have

-   the leader *P* in round *h* is honest,

-   the first honest replica *Q* to enter round h does so at time t, and

-   the network is 5-synchronous at times t and t + *5* + Am(0).

Then the block proposed by P in round h will be finalized.

Here is a proof of the liveness invariant:

1.  Under partial synchrony at time t, all honest replicas will enter
    > round h before time t + 5 (the notarization that ended round h ---
    > 1 for Q as well as the random beacon for round h random will
    > arrive at all honest replicas before this time).

2.  The leader P in round h will propose a block *B* before time t + 5 +
    > Am(0), and again by partial synchrony, this block proposal will be
    > delivered to all other replicas before time t + 25 + Am(0).

3.  Since An(1) \> Am(0) + 25, the protocol logic guarantees that each
    > honest replica supports the notarization of block B and no other
    > block, and thus B will become notarized and finalized.

**5.12 Other issues**

1.  Growth latency

Under a partial synchrony assumption, we can also formulate and prove a
quantitative version of the growth invariant. For simplicity, assume
that the delay functions are defined as recommended above: Am(r) = *25r*
and An(r) = *25r* + e, and further assume that e \< 5. Suppose that at
time t, the highest numbered round entered by any honest replica is h.
Let *r\** be the rank of the lowest ranked honest replica P\* in round
h. Finally, suppose that the network is 5-synchronous at all times in
the interval \[t, t + (3r\* + 2)5\]. Then all honest replicas will start
round h + 1 before time t + 3(r\* + 1)5.

2.  Locally adjusted delay functions

When a replica does not see any finalized blocks for several rounds, it
will start increasing its own delay function An for notarization.
Replicas need not agree on these locally adjusted notarization delay
functions.

Also, while replicas do not explicitly adjust the delay function Ap, we
can mathemati­cally model local clock drift by locally adjusting both
delay functions.

Thus, there are many delay functions, parameterized by replica and
round. The critical condition An(1) \> Am(0) + 25 needed for liveness
then becomes maxAn(1) \> minAm(0) + 25, where the max and min are taken
over all the honest replicas in a given round. Thus, if finalization
fails for enough rounds, all honest replicas will eventually increase
their no­tarization delay until this holds and finalization will then
resume. If some honest replicas increase their notarization latency
function more than other replicas, there is no penalty in terms of
liveness (but there may be in terms of growth latency).

3.  Fairness

Another property that is important in consensus protocols is
**fairness**. Rather than give a general definition, we simply observe
that the liveness invariant also implies a useful fairness property.
Recall that the liveness invariant basically says that in any round
where the leader is honest and the network is synchronous, then the
block proposed by the leader will be finalized. In those rounds where
this happens, the fact that the leader is honest ensures that it will
include in the payload of its block all of the inputs it knows about
(modulo limits on the payload size). So, very roughly speaking, any
input that is disseminated to enough replicas will be included in a
finalized block in a reasonable amount of time with high probability.

6 Message Routing Layer

As discussed in [Section 1.7,](\l) basic computational unit in the IC is
called a **canister**, which is roughly the same as the notion of a
*process,* in that it comprises both a *program* and its *state.* The IC
provides a run-time environment for executing programs in a canister,
and to communicate with other canisters and external users (via message
passing).

The consensus layer (see [Section 5)](\l) bundles inputs into
**payloads**, which get placed into **blocks**, and as blocks are
finalized, the corresponding payloads are delivered to the **message
routing** layer, then processed by the **execution environment**, which
updates the state of the canisters on the replicated state machine and
generates outputs, and these outputs are processed by the **message
routing** layer.

It is useful to distinguish between two types of inputs:

**ingress messages:** these are messages from external users;

**cross-subnet messages:** these are messages from canisters on other
subnets.

We can also distinguish between two types of outputs:

**ingress message *responses****:* these are responses to ingress
messages (which may be re­trieved by external users);

**cross-subnet messages:** these are messages to canisters on other
subnets.

Upon receiving a payload from consensus, the inputs in that payload are
placed into various **input queues**. For each canister *C* running on a
subnet, there are several in­put queues: one for ingress messages to C,
and for each other canister *C* with whom C communicates, one for
cross-subnet messages to C from *CL*

As described below in more detail, in a each round, the execution layer
will consume some of the inputs in these queues, update the replicated
state of the relevant canisters, and place outputs in various **output queues**. For each canister *C*
running on a subnet, there are several output queues: for each other
canister *CC* with whom C communicates, one for cross-subnet messages to
*C* from C. The message routing layer will take the messages in these
output queues and place them into **subnet-to-subnet streams** to be
processed by an **crossnet transfer protocol**, whose job it is to
actually transport these messages to other subnets.

![image](https://user-images.githubusercontent.com/98205496/150630991-0f05f1ec-fe3d-4cb2-8ea8-d8919a8e29e2.png)

Figure 3: Message routing and execution layers

In addition to these output queues, there is also an **ingress history**
data structure. Once an ingress message has been processed by a
canister, a **response** to that ingress message will be recorded in
this data structure. At that point, the external user who provided the
ingress message will be able to retrieve the corresponding response.
(Note that *ingress history* does not maintain the full history of all
ingress messages.)

We also should mention that in addition to cross-subnet messages, there
are also **intra­subnet messages**, which are messages from one canister
to another on the *same subnet.* The message routing layer moves such
messages directly from output queues to correspond­ing input queues.

[Figure 3](\l) illustrates the basic functionality of the message
routing and execution layers.

Note that the replicated state comprises the the state of the canisters,
as well as "system state", including the above-mentioned queues and
streams, as well as the *ingress history* data structure. Thus, both the
message routing and execution layers are involved in updating and
maintaining the replicated state of a subnet. It is essential that all
of this state is updated in a completely *deterministic* fashion, so
that all replicas maintain *exactly* the same state.

Also note that the consensus layer is decoupled from the message routing
and execution layers, in the sense that any forks in the consensus
blockchain are resolved before their payloads are passed to message
routing, and in fact, consensus does not have to keep in

![image](https://user-images.githubusercontent.com/98205496/150631072-14a715f1-d2ae-45b1-8e58-13d14c435e8b.png)

> Figure 4: Per-round certified state organized as a tree

lock step with message routing and consensus and is allowed to run a bit
ahead.

6.1 Per-round certified state

In each round, *some* of the state of a subnet will be *certified.* The
**per-round certified state** is certified using chain-key cryptography
(see [Section 1.6)](\l), specifically, using the (n --- *f* )-out-of-n
threshold signature scheme mentioned in [Section 3](\l). In more detail,
after each replica generates the per-round certified state for a given
round, it will generate a share of the corresponding threshold signature
and broadcast this to all other replicas in its subnet. Upon collecting
*n --- f* such shares, each replica can construct the resulting
threshold signature, which serves as the **certificate** for the
per-round certified state for that round. Note that before signing, the
per-round certified state is hashed as a **Merkle tree**
[\[Mer87\]](\l).

The *per-round certified state* in a given round consists of

1.  *cross-subnet messages* that were recently added to the
    subnet-to-subnet streams;

2.  other metadata, including the *ingress history* data structure;

3.  the *Merkle-tree root hash* of the per-round certified state from
    the previous round.

Note that the per-round certified state does *not* include the entire
replicated state of a subnet, as this in general will be quite huge and
it would be impractical to certify *all* of this state in *every*
round[.[^6]](\l)

[Figure 4](\l) illustrates how the per-round certified state may be
organized into a tree. The first branch of the tree stores various
metadata about each canister (but not the entire replicated state of the
canister). The second branch stores the *ingress history* data
struc­ture. The third branch stores information about the
subnet-to-subnet streams, including a "window" of recently added
cross-subnet messages for each stream. The other branches store other
types of metadata, not discussed here. This tree structure may then be
hashed into a Merkle tree, which has essentially the same size and shape
as this tree.

Per-round certified state is used in several ways in the IC:

-   **Output authentication.** Cross-subnet messages and responses to
    > ingress messages are authenticated using per-round certified
    > state. Using the Merkle tree structure, an individual output
    > (cross-subnet message or ingress message response) may be
    > authen­ticated to any party by providing a threshold signature on
    > the root of the Merkle tree, along with hash values on (and
    > adjacent to) the path in the Merkle from the root to the leaf
    > representing that output. The number if hash values needed to
    > authenticate an individual output is therefore proportional to the
    > *depth* of the Merkle tree, which is typically quite small, even
    > if the *size* of the Merkle tree is very large. Thus, a single
    > threshold signature can be used to efficiently authenticate many
    > individual outputs.

-   **Preventing and detecting non-determinism.** Consensus guarantees
    > that each replica processes inputs in the same order. Since each
    > replica processes these in­puts deterministically, each replica
    > should obtain the same state. However, the IC is designed with an
    > extra layer of robustness to prevent and detect any (accidental)
    > non-deterministic computation, should it arise. The per-round
    > certified state is one of the mechanisms used to do this. Since we
    > use an (n --- *f* )-out-of-n threshold signature for
    > certification, and since f \< n/3, there can only be one sequence
    > of states that is certified.

> To see why state chaining is important, consider the following
> example. Suppose we have 4 replicas, *Pi, P2, P3*, P4, and one is
> corrupt, say P4. Each of the replicas *Pi*,\^2, *P3* start out in the
> same state.
>
> **-**In round 1, because of a non-deterministic computation, *Pi,* P2
> compute a mes­sage *mi* to send to subnet *A,* while P2 computes a
> message to send to subnet A.
>
> **-**In round 2, Pi,P3 compute a message m2 to send to subnet *B*,
> while *P2* computes a message m2 to send to subnet B.
>
> **-**In round 3, P2, P3 compute a message m3 to send to subnet *C*,
> while *P2* computes a message m3 to send to subnet C.

This is illustrated in the following table:

![image](https://user-images.githubusercontent.com/98205496/150631082-9f852891-0498-4046-b672-5d7127e2780b.png)

Pi mi [t]{.smallcaps} A m2 [t]{.smallcaps} B m3 [t]{.smallcaps} C

P2 mi [t]{.smallcaps} A m2 [t]{.smallcaps} B m3 [t]{.smallcaps} C

P3 mi [t]{.smallcaps} A m2 [t]{.smallcaps} B m3 [t]{.smallcaps} C

> We are assuming that replicas Pi, *P2,* and *P3* each individually
> perform a *valid* se­quence of computations, but that because of
> non-determinism, these sequences are not identical. (Even though there
> is not supposed to be any non-determinism, in this example, we are
> supposing that there is.)
>
> Now suppose we did not chain the states. Because *P4* is corrupt and
> may sign anything, he could create a 3-out-of-4 signature on a round-1
> state that says \"mi [t]{.smallcaps} A"，and similarly on a round-2
> state that says *"m2* T B"，and on a round-3 state that says *"m3 T
> C*"，*even though the corresponding sequence*

mi T A, *m2 T* B, m3 T C

> *may not compatible with **any** valid sequence of computations.*
> Worse yet, such an invalid sequence of computations could then lead to
> inconsistent states on other sub­nets.
>
> By chaining, we ensure that even if there is some non-determinism, any
> sequence of certified states corresponds to *some* valid sequence of
> computations that was actually carried out by honest replicas.
>
> • **Coordination with consensus.** The per-round certified state is
> also used to coor­dinate the execution and consensus layers, in two
> different ways:

-   *Consensus throttling.* Each replica will keep track the latest
    > round for which it is has a certified state --- this is called the
    > **certified height**. It will also keep track of the latest round
    > for which it is has a notarized block --- this is called the
    > **notarized height**. If the notarized height is significantly
    > greater than the certified height, this is a signal that execution
    > is lagging consensus, and that consensus needs to be *throttled*.
    > This lagging could be due to non-deterministic computation, or it
    > could just be due to a more benign performance mismatch between
    > the layers. Consensus is throttled by means of the *delay
    > functions* discussed in [Section 5.9](\l) --- specifically, each
    > replica will increase the "governor" value e as the gap between
    > notarized height and certified height grows (this makes use of the
    > notion of "locally adjusted delay functions, as in [Section
    > 5.12.2)](\l).

-   *State-specific payload validation.* As discussed in [Section
    > 5.7](\l), the inputs in a pay­load must pass certain validity
    > checks. In fact, these validity checks may depend to a certain
    > degree on the state. A detail we skipped is that each block
    > includes a round number, with the understanding that these
    > validity checks should be made with respect to the certified state
    > for that round number. A replica that needs to perform this
    > validation will wait until the state for that round number has
    > been certified, and then use the certified state for that round to
    > perform the validation. This ensures that even with
    > non-deterministic computation, all replicas are performing the
    > same validity tests (as otherwise, consensus could get stuck).

6.2 Query calls vs update calls

As we have described it so far, an ingress messages must pass through
consensus so that they are processed in the same order by all replicas
on a subnet. However, an important optimization is available to those
ingress messages whose processing does not modify the replicated state
of a subnet. These are called **query calls** --- as opposed to other
ingress messages, which are called **update calls**. Query calls are
allowed to perform computations which read and possibly update the state
of a canister, but any updates to the state of a canister are never
committed to the replicated state. As such, a query call may be
processed

directly by a single replica without passing through consensus, which
greatly reduces the latency for obtaining a response from a query call.

Note that a response to a query call is not recorded in the *ingress
history* data structure. As such, we cannot directly use the per-round
certified state mechanism to authenticate responses to query calls.
However, a separate mechanism for authenticating such responses is
provided: **certified variables**. As a part of the per-round certified
state, each canister on a subnet is allocated a small number of bytes,
which is the *certified variable for that canister,* whose value may be
updated via update calls, and may be authenticated using the per-round
certified state mechanism. Moreover, a canister may use its certified
variable to store a root of a Merkle tree. In this way, a response to a
query call to a canister may be authenticated so long the response is a
leaf in the Merkle tree rooted at the certified variable for that
canister.

**6.3 External user authentication**

One of the main differences between an ingress message and a
cross-subnet message is the mechanism used for authenticating these
messages. We have already seen above (see [Section 6.1)](\l) how
threshold signatures are used to authenticate cross-subnet messages. The
NNS registry (see [Section 1.5)](\l) holds the public verification keys
for the threshold signatures used to authenticate cross-subnet messages.

There is no central registry for external users. Rather, an external
user identifies himself to a canister using a **user identifier**, which
is a hash of a public signature-verification key. The user holds a
corresponding secret signing key, which is used to sign ingress
messages. Such a signature, as well as the corresponding public key, is
sent along with the ingress message. The IC automatically authenticates
the signature and passes the user identifier to the appropriate
canister. The canister may then authorize the requested operation, based
on the user identifier and other parameters to the operation specified
in the ingress message.

First-time users generate a key pair and derive their user identifier
from the public key during their first interaction with the IC.
Returning users are authenticated using the secret key that is stored by
the user agent. A user may associate several key pairs with a single
user identity, using signature delegation. This is useful, as it allows
a single user to access the IC from several devices using the same user
identity.

7 Execution Layer

The execution environment processes one input at a time. This input is
taken from one of the input queues, and is directed to one canister.
Based on this input and the state of the canister, the execution
environment updates the state of the canister, and additionally may add
messages to output queues and update the *ingress history* (possibly
with a response to an earlier ingress message).

In a given round, the execution environment will process several inputs.
A **scheduler** determines which inputs are executed in a given round,
and in which order. Without going into all the details of the scheduler,
we highlight some of the goals:

• it must be *deterministic,* i.e., only depend on the given data;

> • it should distribute workloads *fairly* among canisters (but
> optimizing for *throughput* over *latency).*
>
> *•* the total amount of work done in each round, measured in terms of
> *cycles* (see [Sec­tion 1.8)](\l), should be close to some
> pre-determined amount.

Another task that the execution environment (together with the message
router) must deal with are situations where a canister on one subnet is
producing cross-subnet messages faster than they can be consumed by a
canister on another subnet. For this, a self-regulating mechanism is
implemented that *throttles* the producing canister.

There are many other resource management and bookkeeping tasks that are
dealt with by the execution environment. However, all of these tasks
must be dealt with *determinis­tically*.

1.  **Random tape**

Each subnet has access to a **distributed pseudorandom generator
(PRG)**. As men­tioned in [Section 3,](\l) pseudorandom bits are derived
from a seed that itself is an *(f* + 1)-out- of-n BLS signature, called
the **Random Tape**. There is a different Random Tape for each round of
the consensus protocol. While this BLS signature is similar to that used
for the Random Beacon used in consensus (see [Section 5.5)](\l), the
mechanics are somewhat different.

In the consensus protocol, as soon as a block at height *h* is
finalized, each honest replica will release its share of Random Tape for
height h + 1. This has two implications:

1.  Before a block at height h is finalized by any honest replica, the
    > Random Tape at height h + 1 is guaranteed to be unpredictable.

2.  By the time block at height h + 1 is finalized by any honest
    > replica, that replica will typically have all the shares it needs
    > to construct the Random Tape at height h + 1.

To obtain pseudorandom bits, a subnet must make a request for these
bits. Such a pseudorandom-bit request will be made as a "system call"
from the execution layer in some round, say h. The system will then
respond to that request later, when the Random Tape of height h + 1 is
available. By property (1) above, it is guaranteed that the requested
pseudorandom bits are unpredictable at the time the request is made. By
property (2) above, the requested random bits will typically be
available at the time the next block is finalized. In fact, in the
current implementation, at the time a block of height h is finalized,
the Consensus Layer (see [Section 5)](\l) will deliver both (the payload
of) the block of height h and the Random Tape of height h + 1
simultaneously to the message routing layer for processing.

8 Chain-key cryptography II: chain-evolution technology

As mentioned in [Section 1.6.2](\l), chain-key cryptography includes a
collection of technologies for robustly and securely maintaining a
blockchain-based replicated state machine over time, which together form
what is called **chain-evolution technology**. Each subnet operates in
**epochs** of many rounds (typically on the order of a few hundreds of
rounds). Chain­evolution technology implements many essential
maintenance activities that are executed periodically with a cadence
that is tied to epochs: *garbage collection, fast forwarding, subnet
membership changes, pro-active resharing of secrets,* and *protocol
upgrades.*

There are two essential ingredients to chain-evolution technology:
**summary blocks** and **catch-up packages (CUPs)**.

1.  **Summary blocks**

The first block in each epoch is a **summary block**. A summary block
contains special data that will be used to manage the shares of the
various threshold signature schemes (see [Section 3)](\l). There are two
threshold schemes:

-   one *(f* + 1)-out-of-n scheme, for which a new signing key is
    generated every epoch;

-   one *(n --- f* )-out-of-n scheme, for which the signing key is
    *reshared* once every epoch.

The low-threshold scheme is used for the *random beacon* and the *random
tape*, while the high-threshold scheme is used to certify the replicated
state of the subnet.

Recall that the DKG protocol (see [Section 3.5)](\l) requires that for
each signing key, we have a set of dealings, and that each replica can
non-interactively obtain its share of the signing key from this set of
dealings.

Also recall that NNS maintains a **registry** that, among other things,
determines the membership of a subnet (see [Section 1.5)](\l). The
registry (and hence the subnet membership) may change over time. Thus,
subnets must agree on which **registry version** they use at various
times for various purposes. This information is also stored in the
summary block.

The summary block for epoch *i* contains the following data fields.

-   *currentRegistryVersion*. This registry version will determine the
    > *consensus committee* used throughout epoch i --- all tasks
    > performed by the consensus layer (block making, notarization,
    > finalization) will be performed by this committee.

-   *nextRegistryVersion*. In each round of consensus, a block maker
    > will include in its proposal the latest registry version it knows
    > about (which must be no earlier than the block the proposed block
    > extends). This ensures that the value *nextRegistryVersion* in the
    > summary block of epoch *i* is fairly up to date.

> The value of *currentRegistryVersion* in epoch *i* is set to the value
> of *nextRegistryVer­sion* in epoch *i* --- 1.

-   *currentDealingSets*. These are the dealing sets that determine the
    > threshold signing keys that will be used to sign messages in epoch
    > *i*.

> As we will see, the *threshold signing committee* for epoch *i* (i.e.,
> the replicas that hold the corresponding threshold signing key shares)
> is the *consensus committee* for epoch i --- 1 .

-   *nextDeolingSets.* This is where dealings that are *collected*
    > during epoch *i ---* 1 are gathered and stored[.[^7]](\l) The
    > value of *currentDealingSets* in epoch i will be set to the value
    > of *nextDealingSets* in epoch i --- 1 (which itself consists of
    > dealings collected in epoch i --- 2).

-   *collectDealingParams.* This describes the parameters that define
    > the dealing sets to be *collected* during epoch i. During epoch i,
    > block makers will include dealings in their proposed blocks that
    > are validated relative to these parameters.

> The *receiving committee* for these dealings is based on the
> *nextRegistryVersion* value of the summary block of epoch i.
>
> For the low-threshold scheme, the *dealing committee* is the
> *consensus committee* for epoch i.
>
> For the high-threshold scheme, the shares to be reshared are based on
> the value of *nextDealingSets* of epoch i. Therefore, the *dealing
> committee* is the *receiving committee* for epoch i --- 1, which is
> also the *consensus committee* for epoch i .
>
> Also observe that the *threshold signing committee* for epoch i is the
> *receiving committee* in epoch i --- 2, which is the *consensus
> committee* for epoch i --- 1 .

Consensus in epoch i relies on the values *currentRegistryVersion* and
*currentDealingSets* in epoch i ------in particular, the makeup of the
consensus committee itself is based on *curren­tRegistryVersion* and the
random beacon used in consensus is based on *currentDealingSets.*
Moreover, just like any other block, there could be more than one
summary block notarized at the beginning of epoch i, and that ambiguity
needs to be resolved by consensus in epoch i. This seeming circularity
is resolved by insisting that a summary block at the beginning of epoch
i --- 1 has been finalized before epoch i starts, since the relevant
values in the newer summary block are copied directly from that older
summary block. This is actually an *implicit synchrony assumption*, but
it is quite an academic assumption. Indeed, because of the "consensus
throttling" discussed in [Section 5.12.2](\l) to ensure liveness, and
because of the length of an epoch is quite large, this can essentially
never happen in practice: long before consensus could reach the end of
epoch i --- 1 without finalizing a summary block for epoch i --- 1, the
notarization delay function would grow to be astronomically large, and
so the partial synchrony assumption needed for finalization will be
satisfied (essentially) with certainty (for all practical
purposes).[[^8]](\l)

2.  **CUPs**

Before describing a CUP, we first point out one detail of random beacon:
the random beacon for each round depends on the random beacon for the
previous round. This is not an essential feature, but it impacts the
design of the CUP.

A **CUP** is a special message (not on the blockchain) that has (mostly)
everything a replica needs to begin working in a given epoch, without
knowing anything about previous epochs. It consists of the following
data fields:

-   The root of a Merkle hash tree for the *entire* replicated state (as
    > opposed to the partial, per-round certified state as in [Section
    > 6.1)](\l).

-   The summary block for the epoch.

-   The random beacon for the first round of the epoch.

-   A signature on the above fields under the *(n --- f* )-out-of-n
    > threshold signing key for the subnet.

To generate a CUP for a given epoch, a replica must wait until the
summary block for that epoch is finalized and the corresponding
per-round state is certified. As already mentioned, the *entire*
replicated state must be hashed as a Merkle tree ------even though a
number of techniques are used to accelerate this process, this is still
quite expensive, which is why it is only done once per epoch. Since a
CUP contains only the root of this Merkle tree, a special **state sync**
subprotocol is used that allows a replica to pull any state that it
needs from its peers ------ again, a number of techniques are used to
accelerate this process, but it is still quite expensive. Since we are
using a high-threshold signature for a CUP, we can be sure that there is
only one valid CUP in any epoch, and moreover, there will be many peers
from which the state may be pulled.

8.3 Implementing chain-evolution technology

**Garbage collection:** Because of the information contained in a CUP
for a given epoch, it is safe for each replica to purge all inputs that
have been processed, and all consensus­level protocol messages needed to
order those inputs, prior to that epoch.

**Fast forwarding:** If a replica in a subnet falls very far behind its
peers (because it is down or disconnected from the network for a long
time), or a new replica is added to a subnet, it can be *fast forwarded*
to the beginning of the most recent epoch, without having to run the
consensus protocol and process all of the inputs up to that point. Such
a replica may do so by obtaining the most recent CUP. Using the summary
block and random beacon contained in the CUP, along with protocol
messages from other replicas (which have not yet been purged), this
replica may run the consensus protocol forward from the beginning of the
corresponding epoch. The replica will also use the state sync
subprotocol to obtain the replicated state corresponding to the
beginning of the epoch, so that it may also process the inputs generated
by consensus.

![image](https://user-images.githubusercontent.com/98205496/150631099-be5afe72-f00b-4a69-836c-cb52f119d0ca.png)

> [Figure 5](\l) illustrates fast forwarding. Here, we assume that a
> replica that needs to catch up has a CUP at the beginning of an epoch,
> which starts (say) at height 101. The CUP contains the root of the
> Merkle tree for the replicated state at height 101, the summary block
> at height 101 (shown in green), and the random beacon at height 101.
> This replica will use the state sync subprotocol to obtain from its
> peers the full replicated state at height 101, using the root of the
> Merkle tree in the CUP to validate this state.
> Having obtained this state, the replica can then participate in the
> protocol, obtaining from its peers blocks (and other messages
> associated with consensus) at heights 102, 103, and so on, and
> updating its copy of the replicated state. If its peers have already
> finalized blocks at greater heights, this replica will process those
> finalized blocks as quickly as it can obtain them (and their
> notarizations and finalizations) from its peers (and as quickly as the
> execution layer will allow).

**Subnet membership changes:** We have already discussed how summary
blocks are used to encode which version of the registry is in force in a
given epoch, and how that is used to determine the subnet membership,
and more specifically, the membership committees for various tasks. Note
that even after a replica is removed from a subnet, it should (if
possible) participate in its assigned committee duties for one
additional epoch .

**Pro-active resharing of secrets:** We have already discussed how
summary blocks are used to generate and reshare signing keys. If
necessary, the required summary block may be obtained from a CUP.

**Protocol upgrades:** CUPs are also used to implement protocol
upgrades. Protocol up­grades are initiated by the NNS (see [Section
1.5)](\l). The basic idea, without going into all the details, is this:

-   when it is time to install a new version of the protocol, the
    > summary block at the beginning of an epoch will indicate this;

-   the replicas running the old version of the protocol will continue
    > running con­sensus long enough to finalize the summary block and
    > to create a corresponding CUP; however, they will create only
    > empty blocks and not pass along any pay­loads to message routing
    > and execution;

> • the new version of the protocol will be installed, and the replicas
> running the new version of the protocol will resume running the full
> protocol from the above CUP.

References

\[AMN+20\] I. Abraham, D. Malkhi, K. Nayak, L. Ren, and M. Yin. Sync
HotStuff: Simple and Practical Synchronous State Machine Replication. In
*2020 IEEE Sympo­sium on Security and Privacy, SP 2020, San Francisco,
CA, USA, May 18-21, 2020,* pages 106-118. IEEE, 2020.

\[BGLS03\] D. Boneh, C. Gentry, B. Lynn, and H. Shacham. Aggregate and
Verifiably Encrypted Signatures from Bilinear Maps. In E. Biham, editor,
*Advances in Cryptology - EUROCRYPT 2003, International Conference on
the Theory and Applications of Cryptographic Techniques, Warsaw, Poland,
May 4-8, 2003, Proceedings,* volume 2656 of *Lecture Notes in Computer
Science,* pages 416­432. Springer, 2003.

\[BKM18\] E. Buchman, J. Kwon, and Z. Milosevic. The latest gossip on
BFT consensus, 2018. arXiv:1807.04938,
<http://arxiv.org/abs/1807.04938>.

\[BLS01\] D. Boneh, B. Lynn, and H. Shacham. Short Signatures from the
Weil Pairing. In C. Boyd, editor, *Advances in Cryptology - ASIACRYPT
2001, 7th Interna­tional Conference on the Theory and Application of
Cryptology and Information Security, Gold Coast, Australia, December
9-13, 2001, Proceedings,* volume 2248 of *Lecture Notes in Computer
Science*, pages 514-532. Springer, 2001.

\[But13\] V. Buterin. Ethereum whitepaper, 2013.
[https://ethereum.org/en/](https://ethereum.org/en/whitepaper/)
[whitepaper/](https://ethereum.org/en/whitepaper/).

\[CDH+21\] J. Camenisch, M. Drijvers, T. Hanke, Y.-A. Pignolet, V.
Shoup, and D. Williams. Internet Computer Consensus. Cryptology ePrint
Archive, Report 2021/632, 2021. <https://ia.cr/2021/632>.

\[CDS94\] R. Cramer, I. Damgard, and B. Schoenmakers. Proofs of Partial
Knowledge and Simplified Design of Witness Hiding Protocols. In
*Advances in Cryptology* -*CRYPTO \'94, 14th Annual International
Cryptology Conference, Santa Bar­bara, California, USA, August 21-25,
1994, Proceedings,* volume 839 of *Lecture Notes in Computer Science*,
pages 174-187. Springer, 1994.

\[CL99\] M. Castro and B. Liskov. Practical Byzantine Fault Tolerance.
In M. I. Seltzer and P. J. Leach, editors, *Proceedings of the Third
USENIX Symposium on Op­erating Systems Design and Implementation (OSDI),
New Orleans, Louisiana, USA, February 22-25, 1999*, pages 173-186.
USENIX Association, 1999.

\[CWA+09\] A. Clement, E. L. Wong, L. Alvisi, M. Dahlin, and M.
Marchetti. Making Byzantine Fault Tolerant Systems Tolerate Byzantine
Faults. In J. Rexford and

> E. G. Sirer, editors, *Proceedings of the 6th USENIX Symposium on
> Networked Systems Design and Implementation, NSDI 2009, April 22-24,
> 2009, Boston, MA, USA,* pages 153-168. USENIX Association, 2009.
> [http://www.usenix.](http://www.usenix.org/events/nsdi09/tech/full_papers/clement/clement.pdf)
> [org/events/nsdi09/tech/full_papers/clement/clement.pdf](http://www.usenix.org/events/nsdi09/tech/full_papers/clement/clement.pdf).

Y. Desmedt. Society and Group Oriented Cryptography: A New Concept. In
C. Pomerance, editor, *Advances in Cryptology - CRYPTO \'87, A
Conference on the Theory and Applications of Cryptographic Techniques,
Santa Barbara, California, USA, August 16-20, 1987, Proceedings,* volume
293 of *Lecture Notes in Computer Science*, pages 120-127. Springer,
1987.

C.  Dwork, N. A. Lynch, and L. J. Stockmeyer. Consensus in the presence
    of partial synchrony. *J. ACM,* 35(2):288-323, 1988.

M. J. Fischer. The Consensus Problem in Unreliable Distributed Systems
(A Brief Survey). In *Fundamentals of Computation Theory, Proceedings of
the 1983 International FCT-Conference, Borgholm, Sweden, August 21-27,
1983,* volume 158 of *Lecture Notes in Computer Science,* pages 127-140.
Springer, 1983.

A. Fiat and A. Shamir. How to Prove Yourself: Practical Solutions to
Identifica­tion and Signature Problems. In *Advances in Cryptology -
CRYPTO \'86, Santa Barbara, California, USA, 1986, Proceedings*, volume
263 of *Lecture Notes in Computer Science*, pages 186-194. Springer,
1986.

Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich. Algorand:
Scaling Byzantine Agreements for Cryptocurrencies. Cryptology ePrint
Archive, Report 2017/454, 2017.
[https://eprint.iacr.org/2017/454.](https://eprint.iacr.org/2017/454)

J. Groth. Non-interactive distributed key generation and key resharing.
Cryp­tology ePrint Archive, Report 2021/339, 2021.
<https://ia.cr/2021/339>.

D.  Johnson, A. Menezes, and S. A. Vanstone. The Elliptic Curve Digital
    Sig­nature Algorithm (ECDSA). *Int. J. Inf. Sec.*, 1(1):36-63, 2001.

R. C. Merkle. A Digital Signature Based on a Conventional Encryption
Func­tion. In *Advances in Cryptology - CRYPTO \'87, A Conference on the
Theory and Applications of Cryptographic Techniques, Santa Barbara,
California, USA, August 16-20, 1987, Proceedings*, volume 293 of
*Lecture Notes in Computer Sci­ence*, pages 369-378. Springer, 1987.

A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song. The Honey Badger of
BFT Protocols. In E. R. Weippl, S. Katzenbeisser, C. Kruegel, A. C.
My­ers, and S. Halevi, editors, *Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, Vienna, Austria,
October 24-28, 2016*, pages 31-42. ACM, 2016.

S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008.
[https:](https://bitcoin.org/bitcoin.pdf)
[//bitcoin.org/bitcoin.pdf](https://bitcoin.org/bitcoin.pdf).

R. Pass and E. Shi. Thunderella: Blockchains with Optimistic Instant
Con­firmation. In J. B. Nielsen and V. Rijmen, editors, *Advances in
Cryptology - EUROCRYPT 2018 - 37th Annual International Conference on
the Theory and Applications of Cryptographic Techniques, Tel Aviv,
Israel, April 29 - May 3, 2018 Proceedings, Part II,* volume 10821 of
*Lecture Notes in Computer Science,* pages 3-33. Springer, 2018.

R. Pass, L. Seeman, and A. Shelat. Analysis of the Blockchain Protocol
in Asynchronous Networks. In *Advances in Cryptology - EUROCRYPT 2017 -
36th Annual International Conference on the Theory and Applications of
Cryp­tographic Techniques, Paris, France, April 30 - May 4, 2017,
Proceedings, Part* II, volume 10211 of *Lecture Notes in Computer
Science,* pages 643-673, 2017.

F. B. Schneider. Implementing Fault-Tolerant Services Using the State
Machine Approach: A Tutorial. *ACM Comput. Surv.,* 22(4):299-319, 1990.

\*<https://dfinity.org/foundation/>

[^1]: 1The IC allows a range of mutability policies fOr smart contracts,
    ranging from purely immutable to unilaterally upgradable, with other
    options in between.

[^2]: See [https://webassembly.org/.](https://webassembly.org/)

[^3]: See
    [https://en.wikipedia.org/wiki/Actor_model.](https://en.wikipedia.org/wiki/Actor_model)

[^4]: See
    [https://en.wikipedia.org/wiki/Persistence\_(computer_science)#Orthogonal_or\_](https://en.wikipedia.org/wiki/Persistence_(computer_science)%23Orthogonal_or_transparent_persistence)

    [transparent_persistence.](https://en.wikipedia.org/wiki/Persistence_(computer_science)%23Orthogonal_or_transparent_persistence)

[^5]: See
    [https://en.wikipedia.org/wiki/Transport_Layer_Security.](https://en.wikipedia.org/wiki/Transport_Layer_Security)

[^6]: But see [Section 8.2](\l)

[^7]: A detail we have omitted is that if we fail to collect all the
    required dealings in epoch *i ---* 1, then as a fallback, the value
    of *nextDealingSets* in epoch i will effectively be set to the value
    of *currentDealingSets* in epoch i. If this happens, then the
    protocol will make use of *dealing committees* and *threshold
    signing committees* from further in the past, as appropriate.

[^8]: Also note that dealings that are collected in epoch i depend on
    data in the summary block for epoch i, in particular, the values of
    *nextDealingSets* and *nextRegistryVersion* . As such, these
    dealings should not be generated and cannot be validated until a
    summary block for epoch i has been finalized.
